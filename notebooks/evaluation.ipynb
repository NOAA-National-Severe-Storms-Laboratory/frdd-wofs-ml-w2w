{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34099d84",
   "metadata": {},
   "source": [
    "## Evaluating the  Models \n",
    "\n",
    "### Primary Goal: Evaluate the ML and BL models\n",
    "\n",
    "In this notebook, I'll provide a brief tutorial on how to evaluate the final machine learning (ML) and baseline (BL) models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85cc0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "#sys.path.append('/home/monte.flora/python_packages/2to6_hr_severe_wx/')\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/ml_workflow/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/VargaPy/')\n",
    "from main.io import load_ml_data, load_bl_data\n",
    "from main.verification import plot_verification \n",
    "from VargaPy.MlUtils import All_Severe, Drop_Unwanted_Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e21be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables (You'll need to change based on where you store your data)\n",
    "FRAMEWORK='POTVIN'\n",
    "TIMESCALE='0to3'\n",
    "data_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c027d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> <b>CAUTION</b> We are evaluating the models, so mode must be set to 'test' in load_ml_data or load_bl_data </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b874f",
   "metadata": {},
   "source": [
    "### Step 1. Load the ML and Baseline Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b9043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35388\n",
      "63121\n",
      "66309\n"
     ]
    }
   ],
   "source": [
    "# Set the target column \n",
    "hazard='tornado' #Use tornado to get 0-2UH Baseline\n",
    "scale='36km' #9,18,36\n",
    "target_col=f'{hazard}_severe__{scale}'\n",
    "# Set the mode == test so as to load the testing dataset. \n",
    "mode='test'\n",
    "All=True #Flag for using all-severe as targets\n",
    "SigSevere=False\n",
    "\n",
    "\n",
    "\n",
    "if All: #Use all severe as target\n",
    "    \n",
    "    X, y, metadata = All_Severe(base_path=data_path, mode=mode, target_scale=scale[0:-2:1], FRAMEWORK=FRAMEWORK, TIMESCALE=TIMESCALE, SigSevere=SigSevere)\n",
    "\n",
    "else:\n",
    "    X,y,metadata = load_ml_data(base_path=data_path, \n",
    "                            mode=mode, \n",
    "                            target_col=target_col)\n",
    "\n",
    "\n",
    "\n",
    "# Load the testing dataset for the baseline model. \n",
    "# The baseline and machine learning models are \n",
    "# using the same target values so we do not need \n",
    "# initialize the baseline target values. \n",
    "bl_df, _, _ = load_bl_data(base_path=data_path, \n",
    "                             mode=mode, \n",
    "                             target_col=target_col, TIMESCALE=TIMESCALE, Big=False\n",
    "                            )\n",
    "bl_col = {'hail_severe' :  'hailcast__nmep_>1_25_45km',\n",
    "          'wind_severe' : 'ws_80__nmep_>40_45km',\n",
    "          'tornado_severe' : 'uh_2to5_instant__nmep_>150_27km' \n",
    "         } #Need to change this?\n",
    "\n",
    "##2-6:\n",
    "#36: 100, 45\n",
    "#18: 100, 27\n",
    "#9: 125, 27\n",
    "\n",
    "\n",
    "\n",
    "#(66309 ,31127)||(15220,6082)\n",
    "# Get the X input into the baseline model. \n",
    "X_bl = bl_df[bl_col[target_col.split('__')[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "115fd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new variables- dropping old 90th percentile\n",
      "(655392, 174)\n",
      "all\n"
     ]
    }
   ],
   "source": [
    "X, ts_suff = Drop_Unwanted_Variables(X, original=False, training_scale=False, intrastormOnly=False, envOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1304f8",
   "metadata": {},
   "source": [
    "### Step 2. Load the ML and Baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f11247c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_path = f'/work/mflora/ML_2TO6HR/models/'\n",
    "base_path=f'/work/samuel.varga/projects/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "model_ind=2\n",
    "\n",
    "# Load the ML model.\n",
    "if False:\n",
    "    ml_data = joblib.load(join(base_path, f'mlModels/isonly/Varga_all_hist_{hazard}_{scale}.joblib'))\n",
    "    # Load the baseline model. \n",
    "    bl_model = joblib.load(join(base_path,f'blModels/{target_col.split(\"_\")[0]}_baseline_model_{scale}.joblib'))\n",
    "elif All:\n",
    "    ml_data=joblib.load(join(base_path, f'mlModels/{scale}/Varga_all_hist_all_{scale}_control_{model_ind}.joblib'))\n",
    "    bl_model=joblib.load(join(base_path, f'blModels/all_baseline_model_{scale}.joblib'))\n",
    "else:\n",
    "    ml_data = joblib.load(join(base_path, f'mlModels/{scale}/{hazard}/Varga_all_hist_{hazard}_{scale}.joblib')) #First Model (Hist)\n",
    "    # Load the baseline model. \n",
    "    bl_model = joblib.load(join(base_path,f'blModels/{target_col.split(\"_\")[0]}_baseline_model_{scale}.joblib'))\n",
    "# When the ML model is saved by the CalibratedPipelineHyperOptCV package, \n",
    "# there are additional metadata that is stored with it. \n",
    "# We want to load the model and the features. \n",
    "# We want to make sure the X input as the features in correct order. \n",
    "ml_model = ml_data['model']\n",
    "features = ml_data['X'].columns\n",
    "\n",
    "ml_models = [ml_model] \n",
    "\n",
    "for name in ['logistic','random']: #Adds the Logistic and random models to the list\n",
    "    #ml_data = joblib.load(join(base_path, f'mlModels/{scale}/{hazard}/Varga_all_{name}_{hazard}_{scale}.joblib'))\n",
    "    ml_data = joblib.load(join(base_path, f'mlModels/{scale}/Varga_all_{name}_all_{scale}_control_{model_ind}.joblib'))\n",
    "    ml_model = ml_data['model']\n",
    "    ml_models.append(ml_model)\n",
    "    \n",
    "names = ['Hist', 'Logistic', 'Random'] \n",
    "estimators = [(name, model) for name, model in zip(names, ml_models)]\n",
    "\n",
    "X = X[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d81adc",
   "metadata": {},
   "source": [
    "## Step 2a. Load HGBT only and baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d67191",
   "metadata": {},
   "source": [
    "### Step 3. Evaluate the ML and Baseline models using Reliability, Performance, and ROC Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n",
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n",
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n",
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n",
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n",
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n",
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n",
      "/home/samuel.varga/miniconda3/envs/Vanilla/lib/python3.10/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  vertices = concatenate([\n"
     ]
    }
   ],
   "source": [
    "# Since X_bl and y are dataframe, we convert them to numpy.arrays \n",
    "# by using .values method. \n",
    "plot_verification(estimators = estimators, \n",
    "                  baseline_estimators = [('BL', bl_model)], \n",
    "                  X = X, \n",
    "                  X_baseline = X_bl.values, \n",
    "                  y=y.values,\n",
    "                  n_boot=10, SRSPlot=False\n",
    "                 )\n",
    "\n",
    "# Uncomment and modify the filename to save the figure. \n",
    "#plt.savefig(f'{target_col}.png')\n",
    "\n",
    "#150 27\n",
    "#75 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6dc3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5720240710902788\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28065177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
