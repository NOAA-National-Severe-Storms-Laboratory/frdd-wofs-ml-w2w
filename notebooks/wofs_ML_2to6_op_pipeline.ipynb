{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bf3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/experiments')\n",
    "sys.path.append('/home/samuel.varga/python_packages/WoF_post')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "from ml_2to6_data_pipeline import GridPointExtracter, get_files\n",
    "from wofs.post.utils import save_dataset, load_multiple_nc_files\n",
    "from os.path import join, exists\n",
    "from glob import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import netCDF4\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98658700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wofs_ml_2to6:\n",
    "    '''\n",
    "    Last Updated: 2024.2.12\n",
    "    ---------------------------------------------------\n",
    "    Pipeline class for WoFS ML for 2-6 hr Lead Times.\n",
    "        - Creates predictors from WoFS Forecast fields\n",
    "        - Saves predictors to target directory (optional)\n",
    "        - produces ML output for specified hazards\n",
    "        - Creates output for multiple predictor scales (optional)\n",
    "        - Saves ML output as NetCDF to target directory\n",
    "    ---------------------------------------------------\n",
    "    Init Arguments:\n",
    "    ncfiles - list - List of ALL forecast output files. Can be of length 53, 49, or 73, but should be ordered\n",
    "    outdir - pathlike - directory to save ML output.\n",
    "    ml_dir - pathlike - directory of ML models.\n",
    "    model_dics - list - list of dictionaries containing info about ML models - See function load_ml_model\n",
    "    baseline_dir - pathlike - directory of BL models. If None, BL predictions will not be created.\n",
    "    out_file - string - filename to save the output netCDF file. Will be joined with ml_dir to create path.\n",
    "    save_predictors - Bool - If true, saves predictors as nc file in outdir.\n",
    "    ml_config - Dict. - defines variables to use as predictors. Leave as None to use default set.\n",
    "    init_time - str - Forecast initialization time as HHmm - Will attempt to parse from files if left as None\n",
    "    verbose - int - If 1, prints basic debug information. If > 1, prints additional debug information.\n",
    "    ---------------------------------------------------\n",
    "    Output:\n",
    "        Returns None if verbose == 0, else returns X, X_bl, metadata, ml_preds, bl_preds\n",
    "    ---------------------------------------------------\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, ncfiles, ml_dir, outdir, model_dics=None, baseline_dir=None, save_predictors=False, ml_config=None, \n",
    "                 init_time=False, out_file=None, verbose=False):\n",
    "        self.forecast_files = ncfiles\n",
    "        self.out_directory = outdir\n",
    "        self.model_directory = ml_dir\n",
    "        self.save_pred = save_predictors\n",
    "        self.verbose = verbose\n",
    "        self.baseline_directory = baseline_dir \n",
    "        \n",
    "        if init_time:\n",
    "            self.init_time = init_time\n",
    "        else:\n",
    "            self.init_time = None\n",
    "        \n",
    "        if out_file:\n",
    "            self.filename = out_file\n",
    "        else:\n",
    "            self.filename = None\n",
    "            \n",
    "        #Define Forecast Fields to Use as Predictors\n",
    "        if ml_config:\n",
    "            self.ml_vars= ml_config\n",
    "        else:\n",
    "            self.ml_vars= { 'ENS_VARS':  ['uh_2to5_instant',\n",
    "                            'uh_0to2_instant',\n",
    "                            'wz_0to2_instant',\n",
    "                            'comp_dz',\n",
    "                            'ws_80',\n",
    "                            'hailcast',\n",
    "                            'w_up',\n",
    "                            'okubo_weiss',\n",
    "                    ],\n",
    "             \n",
    "              'ENV_VARS' : ['mid_level_lapse_rate', \n",
    "                            'low_level_lapse_rate', \n",
    "                           ],\n",
    "             \n",
    "              'SVR_VARS': ['shear_u_0to1', \n",
    "                        'shear_v_0to1', \n",
    "                        'shear_u_0to6', \n",
    "                        'shear_v_0to6',\n",
    "                        'shear_u_3to6', \n",
    "                        'shear_v_3to6',\n",
    "                        'srh_0to3',\n",
    "                        'cape_ml', \n",
    "                        'cin_ml', \n",
    "                        'stp',\n",
    "                        'scp',\n",
    "                       ]\n",
    "            }\n",
    "        \n",
    "        #Optimized NMEP Values for the Baseline Models\n",
    "        self.bl_columns={'hail_severe' :  'hailcast__nmep_>1_25_45km',\n",
    "          'wind_severe' : 'ws_80__nmep_>50_45km',\n",
    "          'tornado_severe' : 'uh_2to5_instant__nmep_>200_45km',\n",
    "            'all_severe' : 'uh_2to5_instant__nmep_>125_45km'}\n",
    "        \n",
    "        if model_dics:\n",
    "            self.model_dics = model_dics\n",
    "        else:\n",
    "            self.model_dics = [{'name':'hist','prefix':'sfe','train_scale':'all','hazard':'all','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'wind','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'hail','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'tornado','target':'36km','suffix':'control','severity':'Sev'}]\n",
    "        \n",
    "        \n",
    "    def load_dataset(self, files):\n",
    "        \"\"\"Loads the forecast files into expected format\"\"\"\n",
    "        \n",
    "        coord_vars = [\"xlat\", \"xlon\", \"hgt\"]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'No. of files: {len(files)}')\n",
    "            print(f'First file: {files[0]}')\n",
    "            print(f'Last file: {files[-1]}')\n",
    "        \n",
    "        #Check to make sure that only timesteps 24-72 are included\n",
    "        if len(files) == 53:\n",
    "            print(f\"Input files are of length 53 instead of 49, dropping files 0-3\") if self.verbose else None\n",
    "            files.sort()\n",
    "            files = files[4:] \n",
    "        elif len(files)==49:\n",
    "            pass\n",
    "        elif len(files) ==73:\n",
    "            print(f\"Input files are of length 73 instead of 49, dropping files 0-23\") if self.verbose else None\n",
    "            files.sort()\n",
    "            files = files[24:]\n",
    "            \n",
    "        else:\n",
    "            print(f'Input files are of unrecognized length {len(files)}')\n",
    "            \n",
    "        print(f'Final input files are of length {len(files)}') if self.verbose else None\n",
    "        \n",
    "        X_strm, coords, _, _  = load_multiple_nc_files(\n",
    "                files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=self.ml_vars['ENS_VARS'])\n",
    "\n",
    "        X_env, _, _, _  = load_multiple_nc_files(\n",
    "                files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=self.ml_vars['ENV_VARS'])\n",
    "\n",
    "        X_svr, _, _, _ = load_multiple_nc_files(\n",
    "                files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=self.ml_vars['SVR_VARS'])\n",
    "\n",
    "        X_env = {**X_env, **X_svr}\n",
    "\n",
    "        X_env = {v : X_env[v][1] for v in X_env.keys()}\n",
    "        X_strm = {v : X_strm[v][1] for v in X_strm.keys()}\n",
    "\n",
    "        ll_grid = (coords['xlat'][1].values, coords['xlon'][1].values)\n",
    "        \n",
    "        \n",
    "        #Update object attributes for later\n",
    "        self.lats = coords['xlat'][1][::3, ::3]\n",
    "        self.lons = coords['xlon'][1][::3, ::3]\n",
    "        if self.filename is None:\n",
    "            self.filename = files[0].split('/')[-1].split('_')\n",
    "            self.filename = f\"{self.filename[0]}_ML2TO6_{'_'.join(self.filename[3:])}\"\n",
    "        if self.init_time is None:\n",
    "            self.init_time = files[0].split('/')[-1].split('_')[-2]\n",
    "    \n",
    "        return X_env, X_strm, files[0], ll_grid\n",
    "    \n",
    "    def load_predictors(self):\n",
    "        '''Loads the files and creates the ML predictors'''\n",
    "        #Load the Data\n",
    "        print('Loading Dataset') if self.verbose else None\n",
    "        X_env, X_strm, ncfile, llgrid  = self.load_dataset(self.forecast_files)\n",
    "        \n",
    "        \n",
    "        self.forecast_shape = np.shape(llgrid[0][::3, ::3])\n",
    "        print(f'Forecast Shape: {self.forecast_shape}') if self.verbose else None\n",
    "\n",
    "        #Create Predictors\n",
    "        \n",
    "        print('Creating Predictors') if self.verbose else None\n",
    "        extracter = GridPointExtracter(ncfile, env_vars=X_env.keys(), strm_vars=X_strm.keys(), ll_grid=llgrid)\n",
    "        df = extracter(X_env, X_strm, predict=True) \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #Convert to Predictor Format\n",
    "        metadata = ['Run Date', 'Init Time','NX','NY']\n",
    "        bl_columns = [b for b in self.bl_columns.values()] if self.baseline_directory else []\n",
    "        ml_features = [f for f in df.columns if f not in metadata]\n",
    "        ml_features = [f for f in ml_features if 'nmep' not in f.lower()]\n",
    "        \n",
    "        if self.verbose >1:\n",
    "            print(f'Metadata Columns: {metadata}')\n",
    "            print(f'BL Columns: {bl_columns}')\n",
    "            print(f'ML Columns: {ml_features}')\n",
    "        \n",
    "        #Separate into different variables\n",
    "        print('Separating Datasets') if self.verbose else None\n",
    "        X = df[ml_features]\n",
    "        X_bl = df[bl_columns] \n",
    "        meta = df[metadata]\n",
    "        \n",
    "        return X, X_bl, meta\n",
    "    \n",
    "    def load_predictor_scales(self, X, training_scale=None, category=None):\n",
    "        '''Takes input dataframe X and removes columns that contain predictors\n",
    "        of a size (category) different than training_scale (category)\n",
    "        ------------------------------------------------------------------\n",
    "        Arguments:\n",
    "        X - input dataframe created by self.load_predictors\n",
    "        training_scale - str/float/int - Only predictors of this scale are kept. Can be any of [None, 9, 27, 45]\n",
    "        category - str - only predictors of this category are kept. Can be any of [None, 'intrastorm', 'environmental']\n",
    "        ------------------------------------------------------------------\n",
    "        Returns:\n",
    "        X - dataframe with n_columns <= Input X\n",
    "        ts_suff - str - training-scale suffix denoting which predictor scales are being used\n",
    "        var_sufs - str - variable suffix denoting which predictor types are being used'''\n",
    "    \n",
    "\n",
    "        #Ensure that NX, NY are not in predictor set (Redundancy check)\n",
    "        X=X[[col for col in X.columns if col not in ['NX','NY']]]\n",
    "\n",
    "        #Remove all columns except for those with correct spatial scale \n",
    "        if training_scale and str(training_scale).lower() !='all': \n",
    "            print(f'Dropping all variables except {training_scale} km fields') if self.verbose else None\n",
    "            X=X[[col for col in X.columns if '{}km'.format(training_scale) in col]] \n",
    "        ts_suff=str(training_scale)+'km' if training_scale != \"all\" else 'all'\n",
    "\n",
    "        #Remove all columns except for those with correct variable type (Storm/Env)\n",
    "        if category and category.lower() !='control': \n",
    "            #Create a list of every column with a storm variable\n",
    "            for storm_var in self.ml_vars['ENS_VARS']:        \n",
    "                storm_columns =[col for col in X.columns if stormvar in col] \n",
    "\n",
    "            print(f'Dropping all {category} variables') if self.verbose else None\n",
    "            X = X[storm_columns] if 'storm' in category.lower() else X.drop(storm_columns, axis=1)\n",
    "\n",
    "        var_suff = category if category else 'control'    \n",
    "        \n",
    "        print(f'Variable Suffix: {var_suff}') if self.verbose else None\n",
    "        print(f'Training Scale: {ts_suff}') if self.verbose else None\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def load_ml_model(self, kwarg_dic):\n",
    "        '''Load naming Parameters based on dict. input\n",
    "            ------------------------------------------\n",
    "            Input: kwarg_dic - dictionary with any of the following k:v pairs:\n",
    "        '''\n",
    "        name=kwarg_dic.get('name', 'hist')\n",
    "        prefix=kwarg_dic.get('prefix', 'sfe_prep')\n",
    "        train_scale=kwarg_dic.get('train', 'all')\n",
    "        hazard_name=kwarg_dic.get('hazard', 'all')\n",
    "        targ_scale=kwarg_dic.get('target', '36km')\n",
    "        suffix=kwarg_dic.get('suffix', 'control')\n",
    "        severity = kwarg_dic.get('severity','Sev')\n",
    "        \n",
    "        if self.verbose >1:\n",
    "            print(f'Loading {prefix}_{train_scale}_{name}_{hazard_name}_{targ_scale}_{severity}_{suffix}_0.joblib')\n",
    "        \n",
    "        ml_data=joblib.load(join(self.model_directory,f'{prefix}_{train_scale}_{name}_{hazard_name}_{targ_scale}_{severity}_{suffix}_0.joblib'))\n",
    "        \n",
    "        out_dic={'model':(name, ml_data['model']), 'suffix':suffix, 'target':targ_scale, 'severity':severity,\n",
    "                 'hazard':hazard_name, 'train':train_scale, 'prefix':prefix, 'name':name, \n",
    "                 'features':ml_data['X'].columns}\n",
    "\n",
    "        return out_dic\n",
    "    \n",
    "    def get_bl_pred(self, X_bl):\n",
    "        '''Loads BL Model and Predicts on X_bl, then reshapes into self.forecast_shape'''\n",
    "\n",
    "        bl_preds = {}\n",
    "\n",
    "        print('Getting BL Predictions') if self.verbose else None\n",
    "\n",
    "        for bl in self.bl_columns.keys():\n",
    "            bl_model = joblib.load(join(self.baseline_directory, f'{bl.split(\"_\")[0]}_baseline_model_36km.joblib'))\n",
    "            bl_pred_2D = bl_model.predict(X_bl[self.bl_columns[bl]]).reshape(self.forecast_shape)\n",
    "            bl_preds[f'{bl}_baseline'] = (['lat','lon'], bl_pred_2D)\n",
    "\n",
    "        return bl_preds\n",
    "    \n",
    "    def get_ml_pred(self, X, model_dic): \n",
    "        '''Takes loaded ML model and predicts on X, then reshapes into self.forecast_shape'''\n",
    "        #Reorganize features to be in correct order\n",
    "        ml_model = model_dic['model'][1]\n",
    "        features = model_dic['features']\n",
    "        X = X[features]\n",
    "\n",
    "        #Make predictions and reshape\n",
    "        ml_pred = ml_model.predict_proba(X)[:,1]  \n",
    "        ml_pred_2D = ml_pred.reshape(self.forecast_shape)\n",
    "\n",
    "        return ml_pred_2D\n",
    "    \n",
    "    def get_predictions(self, X, X_baseline):\n",
    "        '''Create ML and BL Predictions'''\n",
    "        \n",
    "        print('Creating Predictions') if self.verbose else None\n",
    "        #ML Predictions\n",
    "        ml_preds = {}\n",
    "        for ml_dic in self.model_dics:\n",
    "            ml_model = self.load_ml_model(ml_dic)  \n",
    "            ml_name = f'{ml_model[\"severity\"]}_{ml_model[\"hazard\"]}_predictor_scale_{ml_model[\"train\"]}_predictor_type_{ml_model[\"suffix\"]}'\n",
    "            ml_preds[ml_name] = (['lat','lon'], \n",
    "                                 self.get_ml_pred(self.load_predictor_scales(X, ml_model['train'], ml_model['suffix']), ml_model))\n",
    "            \n",
    "        #BL Predictions\n",
    "        if self.baseline_directory:\n",
    "            bl_preds = self.get_bl_pred(X_baseline)\n",
    "        else:\n",
    "            bl_preds = {}\n",
    "        \n",
    "        return ml_preds, bl_preds\n",
    "    \n",
    "    def save_ml_predictions(self, ml, bl):\n",
    "        '''Saves the predictions as an NC file in self.out_directory'''\n",
    "\n",
    "        ds = xarray.Dataset({**ml, **bl, 'xlat':self.lats, 'xlon':self.lons})\n",
    "        ds.to_netcdf(join(self.out_directory, self.filename))\n",
    "        ds.close()\n",
    "        print(f'file saved at {join(self.out_directory, self.filename)}') if self.verbose else None\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def save_predictors(self, X, X_bl, meta):\n",
    "        '''Saves the predictors and metadata as a dataframe in self.out_directory'''\n",
    "        df = pd.concat([X, X_bl, meta], axis=1)\n",
    "        name_split = self.filename.split('ML2TO6')\n",
    "        out_name = f\"{name_split[0]}ML2TO6_PREDICTORS{name_split[1]}\"\n",
    "        df.to_feather(join(self.out_directory, out_name))\n",
    "\n",
    "        print(f'Saving predictors at {join(self.out_directory, out_name)}') if self.verbose else None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        print(f'Running pipeline for {self.init_time}') if self.verbose else None\n",
    "                               \n",
    "        #Load forecast files and create predictors\n",
    "        X, X_bl, meta = self.load_predictors()\n",
    "        \n",
    "        #Get ML and BL predictions\n",
    "        ml_preds, bl_preds = self.get_predictions(X, X_bl)\n",
    "            \n",
    "        self.save_ml_predictions(ml_preds, bl_preds)\n",
    "        \n",
    "        \n",
    "        if self.save_pred:\n",
    "            self.save_predictors(X, X_bl, meta)\n",
    "        \n",
    "        return X, X_bl, meta, ml_preds, bl_preds if self.verbose else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101773b3",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235b4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=get_files('/work/mflora/SummaryFiles/20230523_d1/0000/')[0]\n",
    "ml_dir = '/work/samuel.varga/projects/2to6_hr_severe_wx/sfe_prep/mlModels/'\n",
    "bl_dir = '/work/samuel.varga/projects/2to6_hr_severe_wx/sfe_prep/blModels/'\n",
    "model_dics = [{'name':'hist','prefix':'sfe','train_scale':'all','hazard':'all','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'wind','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'hail','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'tornado','target':'36km','suffix':'control','severity':'Sev'},\n",
    "                          {'name':'hist','prefix':'sfe','train_scale':'45','hazard':'wind','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'45','hazard':'hail','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'45','hazard':'tornado','target':'36km','suffix':'control','severity':'Sev'},\n",
    "                          {'name':'hist','prefix':'sfe','train_scale':'27','hazard':'wind','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'27','hazard':'hail','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'27','hazard':'tornado','target':'36km','suffix':'control','severity':'Sev'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da6e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for None\n",
      "Loading Dataset\n",
      "No. of files: 49\n",
      "First file: /work/mflora/SummaryFiles/20230523_d1/0000/wofs_ALL_24_20230524_0000_0200.nc\n",
      "Last file: /work/mflora/SummaryFiles/20230523_d1/0000/wofs_ALL_72_20230524_0000_0600.nc\n",
      "Final input files are of length 49\n",
      "Forecast Shape: (100, 100)\n",
      "Creating Predictors\n",
      "Separating Datasets\n",
      "Creating Predictions\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n",
      "Variable Suffix: control\n",
      "Training Scale: all\n"
     ]
    }
   ],
   "source": [
    "obj = wofs_ml_2to6(files, ml_dir, '.', model_dics=model_dics, verbose=1, baseline_dir = bl_dir, save_predictors=True)\n",
    "X, X_bl, meta, pred, bl_preds = obj.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2392a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
