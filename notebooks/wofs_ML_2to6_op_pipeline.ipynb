{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bf3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/experiments')\n",
    "sys.path.append('/home/samuel.varga/python_packages/WoF_post')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "from ml_2to6_data_pipeline import GridPointExtracter, get_files\n",
    "from wofs.post.utils import save_dataset, load_multiple_nc_files\n",
    "from os.path import join, exists\n",
    "from glob import glob \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8fc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bl_pred(X_bl, hazard, scale='36km'):\n",
    "    '''Loads BL Model and Predicts on X_bl, then reshapes into self.forecast_shape'''\n",
    "    # Load the baseline model. \n",
    "    bl_model = joblib.load(join(self.baseline_directory,f'{hazard}_baseline_model_{scale}.joblib'))\n",
    "    bl_pred = bl_model.predict(X_bl)\n",
    "    bl_pred_2D = bl_pred.reshape(self.forecast_shape)\n",
    "    \n",
    "    return bl_pred_2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98658700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn this into a class?\n",
    "class wofs_ml_2to6:\n",
    "    '''\n",
    "    Last Updated: 2024.2.12\n",
    "    ---------------------------------------------------\n",
    "    Pipeline class for WoFS ML for 2-6 hr Lead Times.\n",
    "        - Creates predictors from WoFS Forecast fields\n",
    "        - Saves predictors to target directory (optional)\n",
    "        - produces ML output for specified hazards\n",
    "        - Creates output for multiple predictor scales (optional)\n",
    "        - Saves ML output as NetCDF to target directory\n",
    "    ---------------------------------------------------\n",
    "    Init Arguments:\n",
    "    ncfiles - list - List of ALL forecast output files. Can be of length 53 or 49\n",
    "    outdir - pathlike - directory to save ML output.\n",
    "    ml_dir - pathlike - directory of ML models.\n",
    "    model_dics - list - list of dictionaries containing info about ML models - See function load_ml_model\n",
    "    baseline_dir - pathlike - directory of BL models. Defaults to ml_dir if left as None.\n",
    "    save_predictors - Bool - If true, saves predictors as nc file in outdir.\n",
    "    ml_config - Dict. - defines variables to use as predictors. Leave as None to use default set.\n",
    "    init_time - str - Forecast initialization time as HHmm - Will attempt to parse from filename if left as None\n",
    "    verbose - int - If 1, prints basic debug information. If > 1, prints additional debug information.\n",
    "    ---------------------------------------------------\n",
    "    Output:\n",
    "        Returns None\n",
    "    ---------------------------------------------------\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, ncfiles, ml_dir, outdir, model_dics=None, baseline_dir=None, save_predictors=False, ml_config=None, \n",
    "                 init_time=False, verbose=False):\n",
    "        self.forecast_files = ncfiles\n",
    "        self.outdir = outdir\n",
    "        self.model_directory = ml_dir\n",
    "        self.save_pred = save_predictors\n",
    "        self.verbose = verbose\n",
    "        self.baseline_directory = baseline_dir if baseline_dir else ml_dir\n",
    "        \n",
    "        #Determine Init Time\n",
    "        if init_time:\n",
    "            self.init_time = init_time\n",
    "        else:\n",
    "            self.init_time = ncfiles[0].split('/')[-1].split('_')[-2]\n",
    "        \n",
    "        #Define Forecast Fields to Use as Predictors\n",
    "        if ml_config:\n",
    "            self.ml_vars= ml_config\n",
    "        else:\n",
    "            self.ml_vars= { 'ENS_VARS':  ['uh_2to5_instant',\n",
    "                            'uh_0to2_instant',\n",
    "                            'wz_0to2_instant',\n",
    "                            'comp_dz',\n",
    "                            'ws_80',\n",
    "                            'hailcast',\n",
    "                            'w_up',\n",
    "                            'okubo_weiss',\n",
    "                    ],\n",
    "             \n",
    "              'ENV_VARS' : ['mid_level_lapse_rate', \n",
    "                            'low_level_lapse_rate', \n",
    "                           ],\n",
    "             \n",
    "              'SVR_VARS': ['shear_u_0to1', \n",
    "                        'shear_v_0to1', \n",
    "                        'shear_u_0to6', \n",
    "                        'shear_v_0to6',\n",
    "                        'shear_u_3to6', \n",
    "                        'shear_v_3to6',\n",
    "                        'srh_0to3',\n",
    "                        'cape_ml', \n",
    "                        'cin_ml', \n",
    "                        'stp',\n",
    "                        'scp',\n",
    "                       ]\n",
    "            }\n",
    "        \n",
    "        #Optimized NMEP Values for the Baseline Models\n",
    "        self.bl_columns={'hail_severe' :  'hailcast__nmep_>1_25_45km',\n",
    "          'wind_severe' : 'ws_80__nmep_>50_45km',\n",
    "          'tornado_severe' : 'uh_2to5_instant__nmep_>200_27km',\n",
    "            'all_severe' : 'uh_2to5_instant__nmep_>150_45km'}\n",
    "        \n",
    "        if model_dics:\n",
    "            self.model_dics = model_dics\n",
    "        else:\n",
    "            self.model_dics = []\n",
    "        \n",
    "        \n",
    "    def load_dataset(self, files):\n",
    "        \"\"\"Loads the forecast files into expected format\"\"\"\n",
    "        \n",
    "        coord_vars = [\"xlat\", \"xlon\", \"hgt\"]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'No. of files: {len(files)}')\n",
    "            print(f'First file: {files[0]}')\n",
    "            print(f'Last file: {files[-1]}')\n",
    "        \n",
    "        #Check to make sure that only timesteps 24-72 are included\n",
    "        if len(files) == 53:\n",
    "            print(f\"Input files are of length 53 instead of 49, dropping files 0-3\") if self.verbose else None\n",
    "            files.sort()\n",
    "            files = files[4:] \n",
    "        elif len(files)==49:\n",
    "            pass\n",
    "        elif len(files) ==73:\n",
    "            print(f\"Input files are of length 73 instead of 49, dropping files 0-23\") if self.verbose else None\n",
    "            files.sort()\n",
    "            files = files[24:]\n",
    "            \n",
    "        else:\n",
    "            print(f'Input files are of unrecognized length {len(files)}')\n",
    "            \n",
    "        print(f'Final input files are of length {len(files)}') if self.verbose else None\n",
    "        \n",
    "        X_strm, coords, _, _  = load_multiple_nc_files(\n",
    "                files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=self.ml_vars['ENS_VARS'])\n",
    "\n",
    "        X_env, _, _, _  = load_multiple_nc_files(\n",
    "                files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=self.ml_vars['ENV_VARS'])\n",
    "\n",
    "        X_svr, _, _, _ = load_multiple_nc_files(\n",
    "                files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=self.ml_vars['SVR_VARS'])\n",
    "\n",
    "        X_env = {**X_env, **X_svr}\n",
    "\n",
    "        X_env = {v : X_env[v][1] for v in X_env.keys()}\n",
    "        X_strm = {v : X_strm[v][1] for v in X_strm.keys()}\n",
    "\n",
    "        ll_grid = (coords['xlat'][1].values, coords['xlon'][1].values)\n",
    "    \n",
    "        return X_env, X_strm, files[0], ll_grid\n",
    "    \n",
    "    def load_predictors(self):\n",
    "        '''Loads the files and creates the ML predictors'''\n",
    "        #Load the Data\n",
    "        print('Loading Dataset') if self.verbose else None\n",
    "        X_env, X_strm, ncfile, llgrid  = self.load_dataset(self.forecast_files)\n",
    "        \n",
    "        \n",
    "        self.forecast_shape = np.shape(llgrid[0][::3, ::3])\n",
    "        print(f'Forecast Shape: {self.forecast_shape}') if self.verbose else None\n",
    "\n",
    "        #Create Predictors\n",
    "        \n",
    "        ##################################################\n",
    "        ###Remove FRAMEWORK and TIMESCALE arguments ######\n",
    "        ##################################################\n",
    "        \n",
    "        print('Creating Predictors') if self.verbose else None\n",
    "        extracter = GridPointExtracter(ncfile, env_vars=X_env.keys(), strm_vars=X_strm.keys(), ll_grid=llgrid, \n",
    "                                      TIMESCALE='2to6', FRAMEWORK='POTVIN')\n",
    "        df = extracter(X_env, X_strm, predict=True) \n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #Convert to Predictor Format\n",
    "        metadata = ['Run Date', 'Init Time','NX','NY']\n",
    "        bl_columns = [b for b in self.bl_columns.values()]\n",
    "        ml_features = [f for f in df.columns if f not in metadata]\n",
    "        ml_features = [f for f in ml_features if 'nmep' not in f.lower()]\n",
    "        \n",
    "        if self.verbose >1:\n",
    "            print(f'Metadata Columns: {metadata}')\n",
    "            print(f'BL Columns: {bl_columns}')\n",
    "            print(f'ML Columns: {ml_features}')\n",
    "        \n",
    "        #Separate into different variables\n",
    "        print('Separating Datasets') if self.verbose else None\n",
    "        X = df[ml_features]\n",
    "        X_bl = df[bl_columns] \n",
    "        meta = df[metadata]\n",
    "        \n",
    "        return X, X_bl, meta\n",
    "    \n",
    "    def load_predictor_scales(self, X, training_scale=None, category=None):\n",
    "        '''Takes input dataframe X and removes columns that contain predictors\n",
    "        of a size (category) different than training_scale (category)\n",
    "        ------------------------------------------------------------------\n",
    "        Arguments:\n",
    "        X - input dataframe created by self.load_predictors\n",
    "        training_scale - str/float/int - Only predictors of this scale are kept. Can be any of [None, 9, 27, 45]\n",
    "        category - str - only predictors of this category are kept. Can be any of [None, 'intrastorm', 'environmental']\n",
    "        ------------------------------------------------------------------\n",
    "        Returns:\n",
    "        X - dataframe with n_columns <= Input X\n",
    "        ts_suff - str - training-scale suffix denoting which predictor scales are being used\n",
    "        var_sufs - str - variable suffix denoting which predictor types are being used'''\n",
    "    \n",
    "\n",
    "        #Ensure that NX, NY are not in predictor set (Redundancy check)\n",
    "        X=X[[col for col in X.columns if col not in ['NX','NY']]]\n",
    "\n",
    "        #Remove all columns except for those with correct spatial scale \n",
    "        if training_scale: \n",
    "            print(f'Dropping all variables except {training_scale} km fields') if self.verbose else None\n",
    "            X=X[[col for col in X.columns if '{}km'.format(training_scale) in col]] \n",
    "        ts_suff=str(training_scale)+'km' if training_scale else 'all'\n",
    "\n",
    "        #Remove all columns except for those with correct variable type (Storm/Env)\n",
    "        if category and category.lower() !='control': \n",
    "            #Create a list of every column with a storm variable\n",
    "            for storm_var in self.ml_vars['ENS_VARS']:        \n",
    "                storm_columns =[col for col in X.columns if stormvar in col] \n",
    "\n",
    "            print(f'Dropping all {category} variables') if self.verbose else None\n",
    "            X = X[storm_columns] if 'storm' in category.lower() else X.drop(storm_columns, axis=1)\n",
    "\n",
    "        var_suff = category if category else 'control'    \n",
    "        \n",
    "        print(f'Variable Suffix: {var_suff}') if self.verbose else None\n",
    "        print(f'Training Scale: {ts_suff}') if self.verbose else None\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def load_ml_model(self, kwarg_dic):\n",
    "        '''Load naming Parameters based on dict. input\n",
    "            ------------------------------------------\n",
    "            Input: kwarg_dic - dictionary with any of the following k:v pairs:\n",
    "        '''\n",
    "        name=kwarg_dic.get('name', 'hist')\n",
    "        prefix=kwarg_dic.get('prefix', 'sfe_prep')\n",
    "        train_scale=kwarg_dic.get('train', 'all')\n",
    "        hazard_name=kwarg_dic.get('hazard', 'all')\n",
    "        targ_scale=kwarg_dic.get('target', '36km')\n",
    "        suffix=kwarg_dic.get('suffix', 'control')\n",
    "        severity = kwarg_dic.get('severity','Sev')\n",
    "        \n",
    "        if self.verbose >1:\n",
    "            print(f'Loading {prefix}_{train_scale}_{name}_{hazard_name}_{targ_scale}_{severity}_{suffix}_0.joblib')\n",
    "        \n",
    "        ml_data=joblib.load(join(self.model_directory,f'{prefix}_{train_scale}_{name}_{hazard_name}_{targ_scale}_{severity}_{suffix}_0.joblib'))\n",
    "        \n",
    "        out_dic={'model':(name, ml_data['model']), 'suffix':suffix, 'target':targ_scale, 'severity':severity,\n",
    "                 'hazard':hazard_name, 'train':train_scale, 'prefix':prefix, 'name':name, \n",
    "                 'features':ml_data['X'].columns}\n",
    "\n",
    "        return out_dic\n",
    "    \n",
    "    #def get_bl_pred(self)\n",
    "    \n",
    "    def get_ml_pred(self, X, model_dic): \n",
    "        '''Takes loaded ML model and predicts on X, then reshapes into self.forecast_shape'''\n",
    "        #Reorganize features to be in correct order\n",
    "        ml_model = model_dic['model'][1]\n",
    "        features = model_dic['features'].values\n",
    "        X = X[features]\n",
    "\n",
    "        #Make predictions and reshape\n",
    "        ml_pred = ml_model.predict_proba(X)[:,1]  \n",
    "        ml_pred_2D = ml_pred.reshape(self.forecast_shape)\n",
    "\n",
    "        return ml_pred_2D\n",
    "    \n",
    "    def get_predictions(self, X, X_baseline):\n",
    "        '''Create ML and BL Predictions'''\n",
    "        #ML Predictions\n",
    "        ml_preds = {}\n",
    "        for ml_dic in self.model_dics:\n",
    "            ml_model = self.load_ml_model(ml_dic)  \n",
    "            ml_name = f'{ml_model[\"severity\"]}_{ml_model[\"hazard\"]}_predictor_scale_{ml_model[\"train\"]}_predictor_type_{ml_model[\"suffix\"]}'\n",
    "            ml_preds[ml_name] = self.get_ml_pred(self.load_predictor_scales(X, ml_model['train'], ml_model['suffix']), ml_model)\n",
    "            \n",
    "        #BL Predictions\n",
    "                               \n",
    "        return ml_preds\n",
    "    \n",
    "    #def save_predictions(self):\n",
    "    #Save Predictions and Lat/Lon as NC file\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        print(f'Running pipeline for {self.init_time}') if self.verbose else None\n",
    "                               \n",
    "        #Load forecast files and create predictors\n",
    "        X, X_bl, meta = self.load_predictors()\n",
    "        ml_preds = self.get_predictions(X, X_bl)\n",
    "            \n",
    "        \n",
    "        return ml_preds\n",
    "        \n",
    "#wofs_LOCALEXPLAIN__all_sig_severe_43_20220524_0305_0335.json\n",
    "#init, valid, date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101773b3",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91501bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=get_files('/work/mflora/SummaryFiles/20230523_d1/0000/', '2to6')[0]\n",
    "ml_dir = '/work/samuel.varga/projects/2to6_hr_severe_wx/sfe_prep/mlModels/36km/'\n",
    "model_dics = [{'name':'hist','prefix':'sfe','train_scale':'all','hazard':'all','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'wind','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'hail','target':'36km','suffix':'control','severity':'Sev'},\n",
    "             {'name':'hist','prefix':'sfe','train_scale':'all','hazard':'tornado','target':'36km','suffix':'control','severity':'Sev'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56279ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for 0000\n",
      "Loading Dataset\n",
      "No. of files: 49\n",
      "First file: /work/mflora/SummaryFiles/20230523_d1/0000/wofs_ALL_24_20230524_0000_0200.nc\n",
      "Last file: /work/mflora/SummaryFiles/20230523_d1/0000/wofs_ALL_72_20230524_0000_0600.nc\n",
      "Final input files are of length 49\n",
      "Forecast Shape: (100, 100)\n",
      "Creating Predictors\n",
      "True and POTVIN\n",
      "False and POTVIN\n",
      "Separating Datasets\n",
      "Dropping all variables except all km fields\n",
      "Variable Suffix: control\n",
      "Training Scale: allkm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['uh_2to5_instant__time_max__45km__ens_mean',\\n       'uh_0to2_instant__time_max__45km__ens_mean',\\n       'wz_0to2_instant__time_max__45km__ens_mean',\\n       'comp_dz__time_max__45km__ens_mean', 'ws_80__time_max__45km__ens_mean',\\n       'hailcast__time_max__45km__ens_mean', 'w_up__time_max__45km__ens_mean',\\n       'okubo_weiss__time_max__45km__ens_mean',\\n       'uh_2to5_instant__time_max__45km__ens_2nd',\\n       'uh_0to2_instant__time_max__45km__ens_2nd',\\n       ...\\n       'shear_v_0to1__time_avg__9km__ens_std',\\n       'shear_u_0to6__time_avg__9km__ens_std',\\n       'shear_v_0to6__time_avg__9km__ens_std',\\n       'shear_u_3to6__time_avg__9km__ens_std',\\n       'shear_v_3to6__time_avg__9km__ens_std',\\n       'srh_0to3__time_avg__9km__ens_std', 'cape_ml__time_avg__9km__ens_std',\\n       'cin_ml__time_avg__9km__ens_std', 'stp__time_avg__9km__ens_std',\\n       'scp__time_avg__9km__ens_std'],\\n      dtype='object', length=174)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mwofs_ml_2to6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfoo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mwofs_ml_2to6.run_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m#Load forecast files and create predictors\u001b[39;00m\n\u001b[1;32m    274\u001b[0m X, X_bl, meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_predictors()\n\u001b[0;32m--> 275\u001b[0m ml_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_bl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ml_preds\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mwofs_ml_2to6.get_predictions\u001b[0;34m(self, X, X_baseline)\u001b[0m\n\u001b[1;32m    259\u001b[0m     ml_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ml_model(ml_dic)  \n\u001b[1;32m    260\u001b[0m     ml_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mml_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseverity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mml_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhazard\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_predictor_scale_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mml_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_predictor_type_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mml_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 261\u001b[0m     ml_preds[ml_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ml_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_predictor_scales\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml_model\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml_model\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mml_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m#BL Predictions\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ml_preds\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mwofs_ml_2to6.get_ml_pred\u001b[0;34m(self, X, model_dic)\u001b[0m\n\u001b[1;32m    244\u001b[0m ml_model \u001b[38;5;241m=\u001b[39m model_dic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    245\u001b[0m features \u001b[38;5;241m=\u001b[39m model_dic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 246\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#Make predictions and reshape\u001b[39;00m\n\u001b[1;32m    249\u001b[0m ml_pred \u001b[38;5;241m=\u001b[39m ml_model\u001b[38;5;241m.\u001b[39mpredict_proba(X)[:,\u001b[38;5;241m1\u001b[39m]  \n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['uh_2to5_instant__time_max__45km__ens_mean',\\n       'uh_0to2_instant__time_max__45km__ens_mean',\\n       'wz_0to2_instant__time_max__45km__ens_mean',\\n       'comp_dz__time_max__45km__ens_mean', 'ws_80__time_max__45km__ens_mean',\\n       'hailcast__time_max__45km__ens_mean', 'w_up__time_max__45km__ens_mean',\\n       'okubo_weiss__time_max__45km__ens_mean',\\n       'uh_2to5_instant__time_max__45km__ens_2nd',\\n       'uh_0to2_instant__time_max__45km__ens_2nd',\\n       ...\\n       'shear_v_0to1__time_avg__9km__ens_std',\\n       'shear_u_0to6__time_avg__9km__ens_std',\\n       'shear_v_0to6__time_avg__9km__ens_std',\\n       'shear_u_3to6__time_avg__9km__ens_std',\\n       'shear_v_3to6__time_avg__9km__ens_std',\\n       'srh_0to3__time_avg__9km__ens_std', 'cape_ml__time_avg__9km__ens_std',\\n       'cin_ml__time_avg__9km__ens_std', 'stp__time_avg__9km__ens_std',\\n       'scp__time_avg__9km__ens_std'],\\n      dtype='object', length=174)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "pred = wofs_ml_2to6(files, ml_dir, 'foo', model_dics=model_dics, verbose=1).run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce894d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['uh_2to5_instant__time_max__45km__ens_mean',\n",
       "       'uh_0to2_instant__time_max__45km__ens_mean',\n",
       "       'wz_0to2_instant__time_max__45km__ens_mean',\n",
       "       'comp_dz__time_max__45km__ens_mean',\n",
       "       'ws_80__time_max__45km__ens_mean',\n",
       "       'hailcast__time_max__45km__ens_mean',\n",
       "       'w_up__time_max__45km__ens_mean',\n",
       "       'okubo_weiss__time_max__45km__ens_mean',\n",
       "       'uh_2to5_instant__time_max__45km__ens_2nd',\n",
       "       'uh_0to2_instant__time_max__45km__ens_2nd',\n",
       "       'wz_0to2_instant__time_max__45km__ens_2nd',\n",
       "       'comp_dz__time_max__45km__ens_2nd',\n",
       "       'ws_80__time_max__45km__ens_2nd',\n",
       "       'hailcast__time_max__45km__ens_2nd',\n",
       "       'w_up__time_max__45km__ens_2nd',\n",
       "       'okubo_weiss__time_max__45km__ens_2nd',\n",
       "       'uh_2to5_instant__time_max__45km__ens_IQR',\n",
       "       'uh_0to2_instant__time_max__45km__ens_IQR',\n",
       "       'wz_0to2_instant__time_max__45km__ens_IQR',\n",
       "       'comp_dz__time_max__45km__ens_IQR',\n",
       "       'ws_80__time_max__45km__ens_IQR',\n",
       "       'hailcast__time_max__45km__ens_IQR',\n",
       "       'w_up__time_max__45km__ens_IQR',\n",
       "       'okubo_weiss__time_max__45km__ens_IQR',\n",
       "       'uh_2to5_instant__time_max__45km__ens_16th',\n",
       "       'uh_0to2_instant__time_max__45km__ens_16th',\n",
       "       'wz_0to2_instant__time_max__45km__ens_16th',\n",
       "       'comp_dz__time_max__45km__ens_16th',\n",
       "       'ws_80__time_max__45km__ens_16th',\n",
       "       'hailcast__time_max__45km__ens_16th',\n",
       "       'w_up__time_max__45km__ens_16th',\n",
       "       'okubo_weiss__time_max__45km__ens_16th',\n",
       "       'uh_2to5_instant__time_max__27km__ens_mean',\n",
       "       'uh_0to2_instant__time_max__27km__ens_mean',\n",
       "       'wz_0to2_instant__time_max__27km__ens_mean',\n",
       "       'comp_dz__time_max__27km__ens_mean',\n",
       "       'ws_80__time_max__27km__ens_mean',\n",
       "       'hailcast__time_max__27km__ens_mean',\n",
       "       'w_up__time_max__27km__ens_mean',\n",
       "       'okubo_weiss__time_max__27km__ens_mean',\n",
       "       'uh_2to5_instant__time_max__27km__ens_2nd',\n",
       "       'uh_0to2_instant__time_max__27km__ens_2nd',\n",
       "       'wz_0to2_instant__time_max__27km__ens_2nd',\n",
       "       'comp_dz__time_max__27km__ens_2nd',\n",
       "       'ws_80__time_max__27km__ens_2nd',\n",
       "       'hailcast__time_max__27km__ens_2nd',\n",
       "       'w_up__time_max__27km__ens_2nd',\n",
       "       'okubo_weiss__time_max__27km__ens_2nd',\n",
       "       'uh_2to5_instant__time_max__27km__ens_IQR',\n",
       "       'uh_0to2_instant__time_max__27km__ens_IQR',\n",
       "       'wz_0to2_instant__time_max__27km__ens_IQR',\n",
       "       'comp_dz__time_max__27km__ens_IQR',\n",
       "       'ws_80__time_max__27km__ens_IQR',\n",
       "       'hailcast__time_max__27km__ens_IQR',\n",
       "       'w_up__time_max__27km__ens_IQR',\n",
       "       'okubo_weiss__time_max__27km__ens_IQR',\n",
       "       'uh_2to5_instant__time_max__27km__ens_16th',\n",
       "       'uh_0to2_instant__time_max__27km__ens_16th',\n",
       "       'wz_0to2_instant__time_max__27km__ens_16th',\n",
       "       'comp_dz__time_max__27km__ens_16th',\n",
       "       'ws_80__time_max__27km__ens_16th',\n",
       "       'hailcast__time_max__27km__ens_16th',\n",
       "       'w_up__time_max__27km__ens_16th',\n",
       "       'okubo_weiss__time_max__27km__ens_16th',\n",
       "       'uh_2to5_instant__time_max__9km__ens_mean',\n",
       "       'uh_0to2_instant__time_max__9km__ens_mean',\n",
       "       'wz_0to2_instant__time_max__9km__ens_mean',\n",
       "       'comp_dz__time_max__9km__ens_mean',\n",
       "       'ws_80__time_max__9km__ens_mean',\n",
       "       'hailcast__time_max__9km__ens_mean',\n",
       "       'w_up__time_max__9km__ens_mean',\n",
       "       'okubo_weiss__time_max__9km__ens_mean',\n",
       "       'uh_2to5_instant__time_max__9km__ens_2nd',\n",
       "       'uh_0to2_instant__time_max__9km__ens_2nd',\n",
       "       'wz_0to2_instant__time_max__9km__ens_2nd',\n",
       "       'comp_dz__time_max__9km__ens_2nd', 'ws_80__time_max__9km__ens_2nd',\n",
       "       'hailcast__time_max__9km__ens_2nd', 'w_up__time_max__9km__ens_2nd',\n",
       "       'okubo_weiss__time_max__9km__ens_2nd',\n",
       "       'uh_2to5_instant__time_max__9km__ens_IQR',\n",
       "       'uh_0to2_instant__time_max__9km__ens_IQR',\n",
       "       'wz_0to2_instant__time_max__9km__ens_IQR',\n",
       "       'comp_dz__time_max__9km__ens_IQR', 'ws_80__time_max__9km__ens_IQR',\n",
       "       'hailcast__time_max__9km__ens_IQR', 'w_up__time_max__9km__ens_IQR',\n",
       "       'okubo_weiss__time_max__9km__ens_IQR',\n",
       "       'uh_2to5_instant__time_max__9km__ens_16th',\n",
       "       'uh_0to2_instant__time_max__9km__ens_16th',\n",
       "       'wz_0to2_instant__time_max__9km__ens_16th',\n",
       "       'comp_dz__time_max__9km__ens_16th',\n",
       "       'ws_80__time_max__9km__ens_16th',\n",
       "       'hailcast__time_max__9km__ens_16th',\n",
       "       'w_up__time_max__9km__ens_16th',\n",
       "       'okubo_weiss__time_max__9km__ens_16th',\n",
       "       'mid_level_lapse_rate__time_avg__45km__ens_mean',\n",
       "       'low_level_lapse_rate__time_avg__45km__ens_mean',\n",
       "       'shear_u_0to1__time_avg__45km__ens_mean',\n",
       "       'shear_v_0to1__time_avg__45km__ens_mean',\n",
       "       'shear_u_0to6__time_avg__45km__ens_mean',\n",
       "       'shear_v_0to6__time_avg__45km__ens_mean',\n",
       "       'shear_u_3to6__time_avg__45km__ens_mean',\n",
       "       'shear_v_3to6__time_avg__45km__ens_mean',\n",
       "       'srh_0to3__time_avg__45km__ens_mean',\n",
       "       'cape_ml__time_avg__45km__ens_mean',\n",
       "       'cin_ml__time_avg__45km__ens_mean',\n",
       "       'stp__time_avg__45km__ens_mean', 'scp__time_avg__45km__ens_mean',\n",
       "       'mid_level_lapse_rate__time_avg__45km__ens_std',\n",
       "       'low_level_lapse_rate__time_avg__45km__ens_std',\n",
       "       'shear_u_0to1__time_avg__45km__ens_std',\n",
       "       'shear_v_0to1__time_avg__45km__ens_std',\n",
       "       'shear_u_0to6__time_avg__45km__ens_std',\n",
       "       'shear_v_0to6__time_avg__45km__ens_std',\n",
       "       'shear_u_3to6__time_avg__45km__ens_std',\n",
       "       'shear_v_3to6__time_avg__45km__ens_std',\n",
       "       'srh_0to3__time_avg__45km__ens_std',\n",
       "       'cape_ml__time_avg__45km__ens_std',\n",
       "       'cin_ml__time_avg__45km__ens_std', 'stp__time_avg__45km__ens_std',\n",
       "       'scp__time_avg__45km__ens_std',\n",
       "       'mid_level_lapse_rate__time_avg__27km__ens_mean',\n",
       "       'low_level_lapse_rate__time_avg__27km__ens_mean',\n",
       "       'shear_u_0to1__time_avg__27km__ens_mean',\n",
       "       'shear_v_0to1__time_avg__27km__ens_mean',\n",
       "       'shear_u_0to6__time_avg__27km__ens_mean',\n",
       "       'shear_v_0to6__time_avg__27km__ens_mean',\n",
       "       'shear_u_3to6__time_avg__27km__ens_mean',\n",
       "       'shear_v_3to6__time_avg__27km__ens_mean',\n",
       "       'srh_0to3__time_avg__27km__ens_mean',\n",
       "       'cape_ml__time_avg__27km__ens_mean',\n",
       "       'cin_ml__time_avg__27km__ens_mean',\n",
       "       'stp__time_avg__27km__ens_mean', 'scp__time_avg__27km__ens_mean',\n",
       "       'mid_level_lapse_rate__time_avg__27km__ens_std',\n",
       "       'low_level_lapse_rate__time_avg__27km__ens_std',\n",
       "       'shear_u_0to1__time_avg__27km__ens_std',\n",
       "       'shear_v_0to1__time_avg__27km__ens_std',\n",
       "       'shear_u_0to6__time_avg__27km__ens_std',\n",
       "       'shear_v_0to6__time_avg__27km__ens_std',\n",
       "       'shear_u_3to6__time_avg__27km__ens_std',\n",
       "       'shear_v_3to6__time_avg__27km__ens_std',\n",
       "       'srh_0to3__time_avg__27km__ens_std',\n",
       "       'cape_ml__time_avg__27km__ens_std',\n",
       "       'cin_ml__time_avg__27km__ens_std', 'stp__time_avg__27km__ens_std',\n",
       "       'scp__time_avg__27km__ens_std',\n",
       "       'mid_level_lapse_rate__time_avg__9km__ens_mean',\n",
       "       'low_level_lapse_rate__time_avg__9km__ens_mean',\n",
       "       'shear_u_0to1__time_avg__9km__ens_mean',\n",
       "       'shear_v_0to1__time_avg__9km__ens_mean',\n",
       "       'shear_u_0to6__time_avg__9km__ens_mean',\n",
       "       'shear_v_0to6__time_avg__9km__ens_mean',\n",
       "       'shear_u_3to6__time_avg__9km__ens_mean',\n",
       "       'shear_v_3to6__time_avg__9km__ens_mean',\n",
       "       'srh_0to3__time_avg__9km__ens_mean',\n",
       "       'cape_ml__time_avg__9km__ens_mean',\n",
       "       'cin_ml__time_avg__9km__ens_mean', 'stp__time_avg__9km__ens_mean',\n",
       "       'scp__time_avg__9km__ens_mean',\n",
       "       'mid_level_lapse_rate__time_avg__9km__ens_std',\n",
       "       'low_level_lapse_rate__time_avg__9km__ens_std',\n",
       "       'shear_u_0to1__time_avg__9km__ens_std',\n",
       "       'shear_v_0to1__time_avg__9km__ens_std',\n",
       "       'shear_u_0to6__time_avg__9km__ens_std',\n",
       "       'shear_v_0to6__time_avg__9km__ens_std',\n",
       "       'shear_u_3to6__time_avg__9km__ens_std',\n",
       "       'shear_v_3to6__time_avg__9km__ens_std',\n",
       "       'srh_0to3__time_avg__9km__ens_std',\n",
       "       'cape_ml__time_avg__9km__ens_std',\n",
       "       'cin_ml__time_avg__9km__ens_std', 'stp__time_avg__9km__ens_std',\n",
       "       'scp__time_avg__9km__ens_std'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove gpe print\n",
    "pred['features'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa8fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
