{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b40649",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18208db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keep this import at the top of your script. It is uses the Intel extension \n",
    "# for scikit-learn, which improves the training speed of machine learning algorithms\n",
    "# in scikit-learn. \n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/ml_workflow/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/VargaPy')\n",
    "\n",
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from os.path import join\n",
    "from hyperopt import hp\n",
    "\n",
    "#Function and Object Imports\n",
    "from main.io import load_ml_data\n",
    "from ml_workflow.tuned_estimator import TunedEstimator, dates_to_groups\n",
    "from VargaPy.MlUtils import All_Severe, Drop_Unwanted_Variables\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dde4c",
   "metadata": {},
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865dc15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55124\n",
      "95612\n",
      "99317\n",
      "Using new variables- dropping old 90th percentile\n",
      "(1712896, 174)\n",
      "all\n"
     ]
    }
   ],
   "source": [
    "TIMESCALE='0to3' #Timescale of the forecast window: 0to3 || 2to6\n",
    "FRAMEWORK='POTVIN'\n",
    "base_path=f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X, y, meta = All_Severe(base_path, mode='train', target_scale=36,\n",
    "                        FRAMEWORK=FRAMEWORK, TIMESCALE=TIMESCALE, Big=True\n",
    "                       )\n",
    "\n",
    "X, _ = Drop_Unwanted_Variables(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894d4b1",
   "metadata": {},
   "source": [
    "## ML Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ebd198",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator=sklearn.ensemble.HistGradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddc1de",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724b22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save these to a file so I can read it in\n",
    "\n",
    "pipeline_arguments = { #Dictionary of arguments for ml_workflow.preprocess.PreProcessingPipeline\n",
    "                        'imputer':'simple', #From sklearn.impute- handles missing data- simple or iterative\n",
    "                        'scaler':'standard', #From sklearn.preprocessing - scales features - standard, robust, minmax\n",
    "                        'pca':None, #From sklearn.decomposition - method of PCA - None, or valid methods\n",
    "                        'resample':None, #imblearn.under/over_sampling - Resamples training folds of KFCV- under, None, over \n",
    "                        'sampling_strategy':None,\n",
    "    #Change the above line to pass sampling strategies to the undersampler-- Need to also change prep.PPP\n",
    "                        'numeric_features':None,\n",
    "                        'categorical_features':None\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeeb35e",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6e794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid= {#Grid of parameters and ranges for the choice of base_estimator\n",
    "    'learning_rate': hp.choice('learning_rate',[0.0001, 0.001, 0.01, 0.1]),\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes',[5, 10, 20, 30, 40, 50]),\n",
    "    'max_depth': hp.choice('max_depth', [4, 6, 8, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf',[5,10,15,20,30, 40, 50]),\n",
    "    'l2_regularization': hp.choice('l2_regularization',[0.001, 0.01, 0.1]),\n",
    "    'max_bins': hp.choice('max_bins',[15, 31, 63, 127])\n",
    "    \n",
    "            }\n",
    "\n",
    "hyperopt_arguments = { #Dictionary of arguments for ml_workflow.hyperparameter_optimizer.HyperOptCV\n",
    "                        'search_space':param_grid,\n",
    "                        'optimizer':'atpe',\n",
    "                        'max_evals':100,\n",
    "                        'patience':10,\n",
    "                        'scorer':None,\n",
    "                        'n_jobs':1,\n",
    "                        'cv':KFold(n_splits=5)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf88f1e",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7089ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These arguments are used for the CV of the model AFTER the hyperopt. has been performed\n",
    "calibration_arguments = {#Dictionary of arguments for sklearn.calibration.CalibratedClassifierCV\n",
    "                        \n",
    "                        #Will this still be date based? there's no way to pass through the date groups - ask M\n",
    "                        'method':'isotonic',\n",
    "                        'cv':KFold(n_splits=5),\n",
    "                        'n_jobs':None,\n",
    "                        'ensemble':True                        \n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf585f7",
   "metadata": {},
   "source": [
    "## Creating Tuned Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75aa6b65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dates\u001b[38;5;241m=\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun Date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_dates\u001b[38;5;241m=\u001b[39m\u001b[43mdates_to_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m te \u001b[38;5;241m=\u001b[39m TunedEstimator(estimator\u001b[38;5;241m=\u001b[39mbase_estimator, pipeline_kwargs\u001b[38;5;241m=\u001b[39mpipeline_arguments, hyperopt_kwargs\u001b[38;5;241m=\u001b[39mhyperopt_arguments, calibration_cv_kwargs\u001b[38;5;241m=\u001b[39mcalibration_arguments)\n\u001b[1;32m      4\u001b[0m te\u001b[38;5;241m.\u001b[39mfit(X, y, groups)\n",
      "File \u001b[0;32m~/python_packages/ml_workflow/ml_workflow/tuned_estimator.py:40\u001b[0m, in \u001b[0;36mdates_to_groups\u001b[0;34m(dates, n_splits)\u001b[0m\n\u001b[1;32m     37\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(dates\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     38\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(unique_dates)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(dates))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np\u001b[38;5;241m.\u001b[39marray_split(unique_dates, n_splits)):\n\u001b[1;32m     42\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[dates\u001b[38;5;241m.\u001b[39misin(group), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "train_dates=meta['Run Date'].apply(str)\n",
    "train_dates=dates_to_groups(train_dates, n_splits=5) #Change d_t_g in tuned_estimator to include a null dataframe\n",
    "te = TunedEstimator(estimator=base_estimator, pipeline_kwargs=pipeline_arguments, hyperopt_kwargs=hyperopt_arguments, calibration_cv_kwargs=calibration_arguments)\n",
    "te.fit(X, y, groups)\n",
    "#te.save(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "##To do:\n",
    "#Get it to run\n",
    "#Change \n",
    "#Save the arguments as a text file, so that I can just change that file\n",
    "#Change PreProcessingPipeline to accept inputs for the under/oversampler\n",
    "#Change dates_to_groups so that it works\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
