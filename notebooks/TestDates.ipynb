{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a999922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/home/monte.flora/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/monte.flora/python_packages/WoF_post')\n",
    "#sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/experiments')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from ml_2to6_data_pipeline import (GridPointExtracter,\n",
    "                                                       subsampler, \n",
    "                                                       load_dataset)\n",
    "from os.path import join\n",
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting code imports \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/')\n",
    "from main.io import load_ml_data, load_bl_data\n",
    "from bayeshist import bayesian_histogram, plot_bayesian_histogram\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "from wofs.plotting.util import decompose_file_path\n",
    "from wofs.verification.lsrs.get_storm_reports import StormReports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01dea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK='POTVIN'; TIMESCALE='0to3'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE) #What scales are available for targets: 9, 15, 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85fcc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20170501', '20170508', '20170517', '20170518', '20170519',\n",
       "       '20170524', '20170525', '20170530', '20180502', '20180504',\n",
       "       '20180509', '20180512', '20180519', '20180524', '20180530',\n",
       "       '20190513', '20190518', '20190521', '20190522', '20200520',\n",
       "       '20200521', '20200526', '20200528', '20210506', '20210510',\n",
       "       '20210513', '20210514', '20210521', '20210523', '20210524',\n",
       "       '20210525'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates=np.sort(pd.unique(metadata['Run Date'])) #Grab the dates in the testing set\n",
    "init_time_='0000'\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d70e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Date  |Wind|Hail|Torn|\n",
      "--------|----|----|----|\n",
      "20170501| 69 | 00 | 00 |\n",
      "20170508| 01 | 06 | 01 |\n",
      "20170517| 109 | 20 | 07 |\n",
      "20170518| 139 | 54 | 10 |\n",
      "20170519| 19 | 22 | 14 |\n",
      "20170524| 10 | 00 | 11 |\n",
      "20170525| 17 | 04 | 00 |\n",
      "20170530| 05 | 12 | 00 |\n",
      "20180502| 77 | 37 | 20 |\n",
      "20180504| 113 | 01 | 03 |\n",
      "20180509| 29 | 08 | 00 |\n",
      "20180512| 11 | 13 | 00 |\n",
      "20180519| 04 | 16 | 01 |\n",
      "20180524| 24 | 16 | 00 |\n",
      "20180530| 24 | 19 | 03 |\n",
      "20190513| 02 | 06 | 00 |\n",
      "20190518| 32 | 10 | 00 |\n",
      "20190521| 10 | 01 | 07 |\n",
      "20190522| 18 | 33 | 21 |\n",
      "20200520| 41 | 18 | 06 |\n",
      "20200521| 05 | 51 | 01 |\n",
      "20200526| 08 | 11 | 02 |\n",
      "20200528| 01 | 01 | 03 |\n",
      "20210506| 14 | 00 | 00 |\n",
      "20210510| 03 | 26 | 00 |\n",
      "20210513| 01 | 13 | 01 |\n",
      "20210514| 14 | 39 | 01 |\n",
      "20210521| 04 | 01 | 00 |\n",
      "20210523| 21 | 10 | 00 |\n",
      "20210524| 01 | 14 | 01 |\n",
      "20210525| 18 | 20 | 00 |\n"
     ]
    }
   ],
   "source": [
    "print('{:^8}|{}|{}|{}|'.format('Date','Wind','Hail','Torn'))\n",
    "print('--------|----|----|----|')\n",
    "for date in dates:\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time_}/wofs_ENS_24*')[0]\n",
    "    # Get the storm reports. \n",
    "    comps = decompose_file_path(indir)\n",
    "    init_time = comps['VALID_DATE']+comps['VALID_TIME']\n",
    "\n",
    "    init_time = comps['VALID_DATE']+comps['INIT_TIME']\n",
    "\n",
    "\n",
    "\n",
    "    report = StormReportLoader(initial_time=init_time, \n",
    "            forecast_length=180,\n",
    "            err_window=15, \n",
    "            reports_path='/work/samuel.varga/data/2to6_hr_severe_wx/reports/STORM_EVENTS_{0}-{0}.csv'.format(str(date)[0:4]),\n",
    "            report_type='NOAA'\n",
    "            )\n",
    "\n",
    "    ds = xr.load_dataset(indir, decode_times=False)\n",
    "    points = report()\n",
    "    print('{}| {:02} | {:02} | {:02} |'.format(date, points['wind'].shape[0], points['hail'].shape[0], points['tornado'].shape[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e10b2b",
   "metadata": {},
   "source": [
    "# 2-6 Hr Test Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3f0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK='POTVIN'; TIMESCALE='2to6'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70f4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20180503', '20180509', '20180511', '20180523', '20190501',\n",
       "       '20190508', '20190518', '20190520', '20190523', '20190525',\n",
       "       '20200504', '20200506', '20200513', '20200519', '20200520',\n",
       "       '20200521', '20200522', '20200528', '20210505', '20210512',\n",
       "       '20210518', '20210520', '20210521', '20210524', '20210526'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates=np.sort(pd.unique(metadata['Run Date'])) #Grab the dates in the testing set\n",
    "init_time_='0000' #20200519, 20180503, 20180511,  20180523, 20190520, 20210505, 20210520, \n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'20170501', '20170508', '20170517', '20170518', '20170519',\n",
    "       '20170524', '20170525', '20170530', '20180502', '20180504',\n",
    "       '20180509', '20180512', '20180519', '20180524', '20180530',\n",
    "       '20190513', '20190518', '20190521', '20190522', '20200520',\n",
    "       '20200521', '20200526', '20200528', '20210506', '20210510',\n",
    "       '20210513', '20210514', '20210521', '20210523', '20210524',\n",
    "       '20210525']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Date  |Wind|Hail|Torn|\n",
      "--------|----|----|----|\n",
      "20180503| 04 | 04 | 05 |\n",
      "20180509| 33 | 09 | 00 |\n",
      "20180511| 00 | 04 | 00 |\n",
      "20180523| 11 | 31 | 01 |\n"
     ]
    }
   ],
   "source": [
    "print('{:^8}|{}|{}|{}|'.format('Date','Wind','Hail','Torn'))\n",
    "print('--------|----|----|----|')\n",
    "for date in dates:\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time_}/wofs_ENS_24*')[0]\n",
    "    # Get the storm reports. \n",
    "    comps = decompose_file_path(indir)\n",
    "    init_time = comps['VALID_DATE']+comps['VALID_TIME']\n",
    "\n",
    "    init_time = comps['VALID_DATE']+comps['INIT_TIME']\n",
    "\n",
    "\n",
    "\n",
    "    report = StormReportLoader(initial_time=init_time, \n",
    "            forecast_length=240,\n",
    "            err_window=15, \n",
    "            reports_path='/work/samuel.varga/data/2to6_hr_severe_wx/reports/STORM_EVENTS_{0}-{0}.csv'.format(str(date)[0:4]),\n",
    "            report_type='NOAA'\n",
    "            )\n",
    "\n",
    "    ds = xr.load_dataset(indir, decode_times=False)\n",
    "    points = report()\n",
    "    print('{}| {:02} | {:02} | {:02} |'.format(date, points['wind'].shape[0], points['hail'].shape[0], points['tornado'].shape[0] ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
