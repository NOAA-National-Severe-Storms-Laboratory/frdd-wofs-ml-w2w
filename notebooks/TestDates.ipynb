{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a999922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup_file: /home/samuel.varga/python_packages/WoF_post/wofs/data/psadilookup.dat\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/samuel.varga/python_packages/WoF_post')\n",
    "#sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/experiments')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from ml_2to6_data_pipeline import (GridPointExtracter,\n",
    "                                                       subsampler, \n",
    "                                                       load_dataset)\n",
    "from os.path import join\n",
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting code imports \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/')\n",
    "from main.io import load_ml_data, load_bl_data\n",
    "from bayeshist import bayesian_histogram, plot_bayesian_histogram\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "from wofs.plotting.util import decompose_file_path\n",
    "from wofs.verification.lsrs.get_storm_reports import StormReports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01dea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20180511', '20180514', '20180521', '20180531', '20190501',\n",
       "       '20190502', '20190508', '20190510', '20190515', '20190520',\n",
       "       '20190521', '20190523', '20190526', '20200506', '20200507',\n",
       "       '20200508', '20200519', '20200529', '20210504', '20210507',\n",
       "       '20210510', '20210513', '20210520', '20210521', '20210523'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRAMEWORK='POTVIN'; TIMESCALE='0to3'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE, Big=False) #What scales are available for targets: 9, 18, 36\n",
    "dates=np.sort(pd.unique(metadata['Run Date']))\n",
    "init_time_='0000'\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85fcc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending init time to predictors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['20180511', '20180514', '20180521', '20180531', '20190501',\n",
       "       '20190502', '20190508', '20190510', '20190515', '20190520',\n",
       "       '20190521', '20190523', '20190526', '20200506', '20200507',\n",
       "       '20200508', '20200519', '20200529', '20210504', '20210507',\n",
       "       '20210510', '20210513', '20210520', '20210521', '20210523'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRAMEWORK='ADAM'; TIMESCALE='0to3'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE, Big=False) #What scales are available for targets: 9, 18, 36\n",
    "dates=np.sort(pd.unique(metadata['Run Date']))\n",
    "init_time_='0000'\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b18049",
   "metadata": {},
   "outputs": [],
   "source": [
    "  '20180524',\n",
    "       '20180530', \n",
    "       '20190513', '20190516', '20190517', '20190520', '20190526',\n",
    "       '20200506', '20200518', '20200522', '20200528', '20210503',\n",
    "       '20210504', '20210510', '20210513', '20210518', '20210528'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d70e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Date  |Wind|Hail|Torn|\n",
      "--------|----|----|----|\n",
      "20180511| 00 | 00 | 00 |\n",
      "20180514| 36 | 68 | 14 |\n",
      "20180521| 06 | 06 | 06 |\n",
      "20180531| 20 | 22 | 00 |\n",
      "20190501| 04 | 08 | 00 |\n",
      "20190502| 04 | 04 | 02 |\n",
      "20190508| 08 | 18 | 00 |\n",
      "20190510| 02 | 00 | 00 |\n",
      "20190515| 14 | 14 | 00 |\n",
      "20190520| 58 | 54 | 18 |\n",
      "20190521| 08 | 00 | 00 |\n",
      "20190523| 38 | 28 | 08 |\n",
      "20190526| 104 | 38 | 28 |\n",
      "20200506| 00 | 00 | 00 |\n",
      "20200507| 04 | 62 | 00 |\n",
      "20200508| 04 | 04 | 00 |\n",
      "20200519| 04 | 06 | 02 |\n",
      "20200529| 02 | 00 | 00 |\n",
      "20210504| 47 | 00 | 00 |\n",
      "20210507| 00 | 00 | 00 |\n",
      "20210510| 02 | 09 | 00 |\n",
      "20210513| 00 | 01 | 00 |\n",
      "20210520| 03 | 03 | 02 |\n",
      "20210521| 02 | 00 | 00 |\n",
      "20210523| 25 | 03 | 00 |\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "print('{:^8}|{}|{}|{}|'.format('Date','Wind','Hail','Torn'))\n",
    "print('--------|----|----|----|')\n",
    "for date in dates:\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time_}/wofs_ENS_24*')[0]\n",
    "    # Get the storm reports. \n",
    "    comps = decompose_file_path(indir)\n",
    "    #init_time = comps['VALID_DATE']+comps['VALID_TIME']\n",
    "\n",
    "    #init_time = comps['VALID_DATE']+comps['INIT_TIME']\n",
    "    start_time=(pd.to_datetime(comps['VALID_DATE']+comps['INIT_TIME'])+dt.timedelta(minutes=int(comps['TIME_INDEX'])*5)).strftime('%Y%m%d%H%M')\n",
    "\n",
    "    forecast_length = 180 if TIMESCALE=='0to3' else 240\n",
    "\n",
    "\n",
    "\n",
    "    report = StormReportLoader(\n",
    "                reports_path = '/work/mflora/LSRS/StormEvents_2017-2022.csv',\n",
    "                report_type='NOAA',\n",
    "                initial_time=start_time, \n",
    "                forecast_length=forecast_length, \n",
    "                err_window=15,               \n",
    "            )\n",
    "\n",
    "    ds = xr.load_dataset(indir, decode_times=False)\n",
    "    points = report()\n",
    "    print('{}| {:02} | {:02} | {:02} |'.format(date, points['wind'].shape[0], points['hail'].shape[0], points['tornado'].shape[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e10b2b",
   "metadata": {},
   "source": [
    "# 2-6 Hr Test Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3f0a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20180511', '20180514', '20180521', '20180531', '20190501',\n",
       "       '20190502', '20190508', '20190510', '20190515', '20190520',\n",
       "       '20190521', '20190523', '20190526', '20200506', '20200507',\n",
       "       '20200508', '20200519', '20200529', '20210504', '20210507',\n",
       "       '20210510', '20210513', '20210520', '20210521', '20210523'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRAMEWORK='POTVIN'; TIMESCALE='2to6'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE, Big=False)\n",
    "dates=np.sort(pd.unique(metadata['Run Date'])) #Grab the dates in the testing set\n",
    "init_time_='0000' #20200519, 20180503, 20180511,  20180523, 20190520, 20210505, 20210520, \n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70f4ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending init time to predictors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['20180511', '20180514', '20180521', '20180531', '20190501',\n",
       "       '20190502', '20190508', '20190510', '20190515', '20190520',\n",
       "       '20190521', '20190523', '20190526', '20200506', '20200507',\n",
       "       '20200508', '20200519', '20200529', '20210504', '20210507',\n",
       "       '20210510', '20210513', '20210520', '20210521', '20210523'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRAMEWORK='ADAM'; TIMESCALE='2to6'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE, Big=False)\n",
    "dates=np.sort(pd.unique(metadata['Run Date'])) #Grab the dates in the testing set\n",
    "init_time_='0000' #20200519, 20180503, 20180511,  20180523, 20190520, 20210505, 20210520, \n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb8734f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (424570557.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [5]\u001b[0;36m\u001b[0m\n\u001b[0;31m    '20170524', '20170525', '20170530', '20180502', '20180504',\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "'20180501', '20180510', '20180515', '20180524', '20180528',\n",
    "       '20180530', '20190501', '20190506', '20190509', '20190510',\n",
    "       '20190513', '20190516', '20190517', '20190520', '20190526',\n",
    "       '20200506', '20200518', '20200522', '20200528', '20210503',\n",
    "       '20210504', '20210510', '20210513', '20210518', '20210528']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e71b83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Date  |Wind|Hail|Torn|\n",
      "--------|----|----|----|\n",
      "20180511| 00 | 00 | 00 |\n",
      "20180514| 36 | 72 | 14 |\n",
      "20180521| 06 | 06 | 06 |\n",
      "20180531| 24 | 22 | 00 |\n",
      "20190501| 04 | 12 | 00 |\n",
      "20190502| 04 | 04 | 02 |\n",
      "20190508| 10 | 22 | 00 |\n",
      "20190510| 02 | 00 | 00 |\n",
      "20190515| 14 | 14 | 00 |\n",
      "20190520| 60 | 54 | 18 |\n",
      "20190521| 10 | 00 | 02 |\n",
      "20190523| 38 | 28 | 08 |\n",
      "20190526| 136 | 42 | 34 |\n",
      "20200506| 00 | 00 | 00 |\n",
      "20200507| 06 | 68 | 00 |\n",
      "20200508| 04 | 04 | 00 |\n",
      "20200519| 04 | 06 | 02 |\n",
      "20200529| 02 | 00 | 00 |\n",
      "20210504| 47 | 00 | 00 |\n",
      "20210507| 00 | 00 | 00 |\n",
      "20210510| 02 | 09 | 00 |\n",
      "20210513| 02 | 01 | 00 |\n",
      "20210520| 03 | 03 | 02 |\n",
      "20210521| 02 | 00 | 00 |\n",
      "20210523| 28 | 03 | 00 |\n"
     ]
    }
   ],
   "source": [
    "print('{:^8}|{}|{}|{}|'.format('Date','Wind','Hail','Torn'))\n",
    "print('--------|----|----|----|')\n",
    "for date in dates:\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time_}/wofs_ENS_24*')[0]\n",
    "    # Get the storm reports. \n",
    "    comps = decompose_file_path(indir)\n",
    "   # init_time = comps['VALID_DATE']+comps['VALID_TIME']\n",
    "\n",
    "    #init_time = comps['VALID_DATE']+comps['INIT_TIME']\n",
    "\n",
    "    start_time=(pd.to_datetime(comps['VALID_DATE']+comps['INIT_TIME'])+dt.timedelta(minutes=int(comps['TIME_INDEX'])*5)).strftime('%Y%m%d%H%M')\n",
    "\n",
    "    forecast_length = 180 if TIMESCALE=='0to3' else 240\n",
    "\n",
    "\n",
    "\n",
    "    report = StormReportLoader(\n",
    "                reports_path = '/work/mflora/LSRS/StormEvents_2017-2022.csv',\n",
    "                report_type='NOAA',\n",
    "                initial_time=start_time, \n",
    "                forecast_length=forecast_length, \n",
    "                err_window=15,               \n",
    "            )\n",
    "\n",
    "    ds = xr.load_dataset(indir, decode_times=False)\n",
    "    points = report()\n",
    "    print('{}| {:02} | {:02} | {:02} |'.format(date, points['wind'].shape[0], points['hail'].shape[0], points['tornado'].shape[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1192be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uh_2to5_instant__time_max__4km__ens_16th',\n",
       "       'uh_0to2_instant__time_max__4km__ens_16th',\n",
       "       'wz_0to2_instant__time_max__4km__ens_16th',\n",
       "       'comp_dz__time_max__4km__ens_16th', 'ws_80__time_max__4km__ens_16th',\n",
       "       'hailcast__time_max__4km__ens_16th', 'w_up__time_max__4km__ens_16th',\n",
       "       'okubo_weiss__time_max__4km__ens_16th',\n",
       "       'uh_2to5_instant__time_max__4km__ens_max',\n",
       "       'uh_0to2_instant__time_max__4km__ens_max',\n",
       "       'wz_0to2_instant__time_max__4km__ens_max',\n",
       "       'comp_dz__time_max__4km__ens_max', 'ws_80__time_max__4km__ens_max',\n",
       "       'hailcast__time_max__4km__ens_max', 'w_up__time_max__4km__ens_max',\n",
       "       'okubo_weiss__time_max__4km__ens_max',\n",
       "       'uh_2to5_instant__time_max__4km__ens_mean__smoothed',\n",
       "       'uh_0to2_instant__time_max__4km__ens_mean__smoothed',\n",
       "       'wz_0to2_instant__time_max__4km__ens_mean__smoothed',\n",
       "       'comp_dz__time_max__4km__ens_mean__smoothed',\n",
       "       'ws_80__time_max__4km__ens_mean__smoothed',\n",
       "       'hailcast__time_max__4km__ens_mean__smoothed',\n",
       "       'w_up__time_max__4km__ens_mean__smoothed',\n",
       "       'okubo_weiss__time_max__4km__ens_mean__smoothed',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_0',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_1',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_2',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_3',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_4',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_5',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_6',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_7',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_8',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_9',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_10',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_11',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_12',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_13',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_14',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_15',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_16',\n",
       "       'uh_2to5_instant__time_max__9km__smoothed_17',\n",
       "       'mid_level_lapse_rate__4km__ens_mean__time_avg',\n",
       "       'low_level_lapse_rate__4km__ens_mean__time_avg',\n",
       "       'shear_u_0to1__4km__ens_mean__time_avg',\n",
       "       'shear_v_0to1__4km__ens_mean__time_avg',\n",
       "       'shear_u_0to6__4km__ens_mean__time_avg',\n",
       "       'shear_v_0to6__4km__ens_mean__time_avg',\n",
       "       'shear_u_3to6__4km__ens_mean__time_avg',\n",
       "       'shear_v_3to6__4km__ens_mean__time_avg',\n",
       "       'srh_0to3__4km__ens_mean__time_avg', 'cape_ml__4km__ens_mean__time_avg',\n",
       "       'cin_ml__4km__ens_mean__time_avg', 'stp__4km__ens_mean__time_avg',\n",
       "       'scp__4km__ens_mean__time_avg', 'NY', 'NX', 'Init Time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc543c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
