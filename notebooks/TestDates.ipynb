{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a999922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/home/monte.flora/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/monte.flora/python_packages/WoF_post')\n",
    "#sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe')\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/experiments')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from ml_2to6_data_pipeline import (GridPointExtracter,\n",
    "                                                       subsampler, \n",
    "                                                       load_dataset)\n",
    "from os.path import join\n",
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting code imports \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/')\n",
    "from main.io import load_ml_data, load_bl_data\n",
    "from bayeshist import bayesian_histogram, plot_bayesian_histogram\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "from wofs.plotting.util import decompose_file_path\n",
    "from wofs.verification.lsrs.get_storm_reports import StormReports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01dea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK='POTVIN'; TIMESCALE='0to3'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE, Big=True) #What scales are available for targets: 9, 18, 36\n",
    "dates=np.sort(pd.unique(metadata['Run Date']))\n",
    "init_time_='0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85fcc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20170501', '20170508', '20170517', '20170518', '20170519',\n",
       "       '20170524', '20170525', '20170530', '20180502', '20180504',\n",
       "       '20180509', '20180512', '20180519', '20180524', '20180530',\n",
       "       '20190513', '20190518', '20190521', '20190522', '20200520',\n",
       "       '20200521', '20200526', '20200528', '20210506', '20210510',\n",
       "       '20210513', '20210514', '20210521', '20210523', '20210524',\n",
       "       '20210525'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates=np.sort(pd.unique(metadata['Run Date'])) #Grab the dates in the testing set\n",
    "init_time_='0000'\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b18049",
   "metadata": {},
   "outputs": [],
   "source": [
    "  '20180524',\n",
    "       '20180530', \n",
    "       '20190513', '20190516', '20190517', '20190520', '20190526',\n",
    "       '20200506', '20200518', '20200522', '20200528', '20210503',\n",
    "       '20210504', '20210510', '20210513', '20210518', '20210528'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d70e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Date  |Wind|Hail|Torn|\n",
      "--------|----|----|----|\n",
      "20180503| 04 | 04 | 04 |\n",
      "20180507| 08 | 06 | 00 |\n",
      "20180512| 11 | 13 | 00 |\n",
      "20180519| 04 | 16 | 01 |\n",
      "20190503| 13 | 06 | 00 |\n",
      "20190508| 19 | 06 | 02 |\n",
      "20190518| 32 | 10 | 00 |\n",
      "20190525| 63 | 19 | 05 |\n",
      "20200519| 03 | 07 | 01 |\n",
      "20200521| 05 | 51 | 01 |\n",
      "20210505| 01 | 01 | 00 |\n",
      "20210524| 01 | 14 | 01 |\n"
     ]
    }
   ],
   "source": [
    "print('{:^8}|{}|{}|{}|'.format('Date','Wind','Hail','Torn'))\n",
    "print('--------|----|----|----|')\n",
    "for date in dates:\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time_}/wofs_ENS_24*')[0]\n",
    "    # Get the storm reports. \n",
    "    comps = decompose_file_path(indir)\n",
    "    init_time = comps['VALID_DATE']+comps['VALID_TIME']\n",
    "\n",
    "    init_time = comps['VALID_DATE']+comps['INIT_TIME']\n",
    "\n",
    "\n",
    "\n",
    "    report = StormReportLoader(initial_time=init_time, \n",
    "            forecast_length=180,\n",
    "            err_window=15, \n",
    "            reports_path='/work/samuel.varga/data/2to6_hr_severe_wx/reports/STORM_EVENTS_{0}-{0}.csv'.format(str(date)[0:4]),\n",
    "            report_type='NOAA'\n",
    "            )\n",
    "\n",
    "    ds = xr.load_dataset(indir, decode_times=False)\n",
    "    points = report()\n",
    "    print('{}| {:02} | {:02} | {:02} |'.format(date, points['wind'].shape[0], points['hail'].shape[0], points['tornado'].shape[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e10b2b",
   "metadata": {},
   "source": [
    "# 2-6 Hr Test Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3f0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK='POTVIN'; TIMESCALE='2to6'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'\n",
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='test', \n",
    "                            target_col='hail_severe__36km',\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE, Big=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70f4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20180501', '20180510', '20180515', '20180524', '20180528',\n",
       "       '20180530', '20190501', '20190506', '20190509', '20190510',\n",
       "       '20190513', '20190516', '20190517', '20190520', '20190526',\n",
       "       '20200506', '20200518', '20200522', '20200528', '20210503',\n",
       "       '20210504', '20210510', '20210513', '20210518', '20210528'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates=np.sort(pd.unique(metadata['Run Date'])) #Grab the dates in the testing set\n",
    "init_time_='0000' #20200519, 20180503, 20180511,  20180523, 20190520, 20210505, 20210520, \n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb8734f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (424570557.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [5]\u001b[0;36m\u001b[0m\n\u001b[0;31m    '20170524', '20170525', '20170530', '20180502', '20180504',\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "'20180501', '20180510', '20180515', '20180524', '20180528',\n",
    "       '20180530', '20190501', '20190506', '20190509', '20190510',\n",
    "       '20190513', '20190516', '20190517', '20190520', '20190526',\n",
    "       '20200506', '20200518', '20200522', '20200528', '20210503',\n",
    "       '20210504', '20210510', '20210513', '20210518', '20210528']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71b83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Date  |Wind|Hail|Torn|\n",
      "--------|----|----|----|\n",
      "20180501| 04 | 25 | 08 |\n",
      "20180510| 23 | 17 | 00 |\n",
      "20180515| 33 | 26 | 00 |\n",
      "20180524| 24 | 16 | 00 |\n",
      "20180528| 28 | 36 | 04 |\n",
      "20180530| 36 | 25 | 03 |\n",
      "20190501| 03 | 10 | 01 |\n",
      "20190506| 41 | 54 | 02 |\n",
      "20190509| 06 | 24 | 00 |\n",
      "20190510| 02 | 00 | 00 |\n",
      "20190513| 02 | 06 | 00 |\n",
      "20190516| 28 | 62 | 00 |\n",
      "20190517| 08 | 28 | 21 |\n",
      "20190520| 32 | 34 | 12 |\n",
      "20190526| 81 | 25 | 20 |\n",
      "20200506| 00 | 00 | 00 |\n",
      "20200518| 02 | 01 | 00 |\n",
      "20200522| 14 | 55 | 08 |\n",
      "20200528| 01 | 02 | 03 |\n",
      "20210503| 37 | 101 | 16 |\n",
      "20210504| 83 | 00 | 01 |\n",
      "20210510| 03 | 27 | 00 |\n",
      "20210513| 01 | 13 | 01 |\n",
      "20210518| 16 | 09 | 02 |\n",
      "20210528| 41 | 27 | 02 |\n"
     ]
    }
   ],
   "source": [
    "print('{:^8}|{}|{}|{}|'.format('Date','Wind','Hail','Torn'))\n",
    "print('--------|----|----|----|')\n",
    "for date in dates:\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time_}/wofs_ENS_24*')[0]\n",
    "    # Get the storm reports. \n",
    "    comps = decompose_file_path(indir)\n",
    "    init_time = comps['VALID_DATE']+comps['VALID_TIME']\n",
    "\n",
    "    init_time = comps['VALID_DATE']+comps['INIT_TIME']\n",
    "\n",
    "\n",
    "\n",
    "    report = StormReportLoader(initial_time=init_time, \n",
    "            forecast_length=240,\n",
    "            err_window=15, \n",
    "            reports_path='/work/samuel.varga/data/2to6_hr_severe_wx/reports/STORM_EVENTS_{0}-{0}.csv'.format(str(date)[0:4]),\n",
    "            report_type='NOAA'\n",
    "            )\n",
    "\n",
    "    ds = xr.load_dataset(indir, decode_times=False)\n",
    "    points = report()\n",
    "    print('{}| {:02} | {:02} | {:02} |'.format(date, points['wind'].shape[0], points['hail'].shape[0], points['tornado'].shape[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1192be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
