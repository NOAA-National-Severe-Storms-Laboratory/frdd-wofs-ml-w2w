{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26222d3",
   "metadata": {},
   "source": [
    "## Visualizing the Model Predictions\n",
    "\n",
    "#### Primary Goal: Visualize and inspect the model predictions \n",
    "In this notebook, I'll brief tutorial for making 2D visualizations of the ML model predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dff73803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "import h5netcdf\n",
    "import xarray as xr\n",
    "from os.path import join\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import shapely\n",
    "import cartopy\n",
    "%matplotlib inline\n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/ml_workflow/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/MontePython/')\n",
    "sys.path.append('/home/monte.flora/python_packages/WoF_post') #WoF post package\n",
    "sys.path.append('/home/samuel.varga/python_packages/wofs_ml_severe/')\n",
    "from main.io import load_ml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53490768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from cartopy.feature import ShapelyFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f246ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wofs.plotting.wofs_colors import WoFSColors\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "from wofs_ml_severe.data_pipeline.storm_report_downloader import StormReportDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22be121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables (You'll need to change based on where you store your data)\n",
    "#base_path = '/work/mflora/ML_2TO6HR/data'\n",
    "TIMESCALE='0to3'\n",
    "FRAMEWORK='POTVIN'\n",
    "date='20210524'\n",
    "All=True\n",
    "#base_path='/work/samuel.varga/data/2to6_hr_severe_wx/singleCase'\n",
    "base_path=f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}/Single'\n",
    "\n",
    "# ['20180509','20190518','20200520','20200521','20200528','20210521','20210524']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e183d",
   "metadata": {},
   "source": [
    "# To Do List:\n",
    "Update Init time label\n",
    "Update threshold label for all severe\n",
    "Update hazard scale and time scale in plot label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946e2bd",
   "metadata": {},
   "source": [
    "### Load the data for the ML model and BL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72dc2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target column and baseline variable. \n",
    "\n",
    "hazard='tornado' #use tornado for UH baseline\n",
    "scale = '36km'\n",
    "if TIMESCALE=='0to3':\n",
    "    Vdate=str(int(date)+1) #Should probably use DT for this\n",
    "    init_time = '0000' #Used to get domain-- is 1900 when making data\n",
    "    title = f'Valid: {Vdate[:4]}-{Vdate[4:6]}-{Vdate[6:]} 00:00 - 03:00 UTC'\n",
    "else:\n",
    "    Vdate=str(int(date)+1)\n",
    "    init_time='2200'\n",
    "    title = f'Valid: {Vdate[:4]}-{Vdate[4:6]}-{Vdate[6:]} 00:00 - 04:00 UTC'\n",
    "\n",
    "\n",
    "bl_column_dict = {'hail' :  'hailcast__nmep_>1_0_45km', \n",
    "                  'wind' :  'ws_80__nmep_>40_45km', \n",
    "                  'tornado' : 'uh_2to5_instant__nmep_>50_27km'\n",
    "                 }\n",
    "bl_column = bl_column_dict[hazard]\n",
    "\n",
    "# Load the testing dataset for the ML model.\n",
    "X, X_bl = load_ml_data(base_path=base_path, \n",
    "                       date=date,  \n",
    "                       bl_column = bl_column, FRAMEWORK=FRAMEWORK, TIMESCALE=TIMESCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15bae678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates=['20180503', '20180509', '20180511', '20180523', '20190501',\n",
    "#       '20190508', '20190518', '20190520', '20190523', '20190525',\n",
    "#       '20200504', '20200506', '20200513', '20200519', '20200520',\n",
    "#       '20200521', '20200522', '20200528', '20210505', '20210512',\n",
    "#       '20210518', '20210520', '20210521', '20210524', '20210526']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890c947",
   "metadata": {},
   "source": [
    "## Selection of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425666d0",
   "metadata": {},
   "source": [
    "The current dataset contains both the old 90th percentile (ens_90th), which was calculated with regression. (See nanpercentile for more information), as well as the new 90th percentile (ens_16th), 10th percentile (ens_2nd), and IQR (ens_iqr) for intrastorm variables. If original is set to true, all new variables are dropped and the old 90th percentile feature is used. If original is set to false, all new variables are used and the old 90th percentile feature is dropped. This allows us to create a control using the original variables, so that we can assess the impact of the new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cb8eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using new variables- dropping old 90th percentile\n"
     ]
    }
   ],
   "source": [
    "original=False\n",
    "if original:\n",
    "    print(\"Using Original Variables- Dropping IQR, 2nd lowest, 2nd highest, and intrastorm mean\")\n",
    "    X=X[[col for col in X.columns if 'IQR' not in col]]\n",
    "    X=X[[col for col in X.columns if '2nd' not in col]]\n",
    "    X=X[[col for col in X.columns if '16th' not in col]]\n",
    "    #Mean of intrastorm vars\n",
    "    vardic={ 'ENS_VARS':  ['uh_2to5_instant',\n",
    "                            'uh_0to2_instant',\n",
    "                            'wz_0to2_instant',\n",
    "                            'comp_dz',\n",
    "                            'ws_80',\n",
    "                            'hailcast',\n",
    "                            'w_up',\n",
    "                            'okubo_weiss',\n",
    "                    ]}\n",
    "    badthings=np.array([])\n",
    "    for strmvar in vardic['ENS_VARS']:\n",
    "        badthings=np.append(badthings, [col for col in X.columns if 'mean' in col and strmvar in col] )\n",
    "\n",
    "    X=X.drop(badthings, axis=1)\n",
    "    \n",
    "else:\n",
    "    print(\"Using new variables- dropping old 90th percentile\")\n",
    "    X=X[[col for col in X.columns if '90th' not in col]] #Keeps all columns except the old 90th %ile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f187dd2",
   "metadata": {},
   "source": [
    "### Load the ML model and BL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92f697bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bl_pred(X_bl, shape, hazard, scale='36km', FRAMEWORK=None, TIMESCALE=None):\n",
    "    if FRAMEWORK and TIMESCALE:\n",
    "        in_path=f'/work/samuel.varga/projects/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}/blModels/'\n",
    "    else:\n",
    "        in_path = '/work/samuel.varga/projects/2to6_hr_severe_wx/blModels/' #Change this to your own path\n",
    "    # Load the baseline model. \n",
    "    bl_model = joblib.load(join(in_path,f'{hazard}_baseline_model_{scale}.joblib')) #Update naming of BL models\n",
    "    bl_pred = bl_model.predict(X_bl)\n",
    "    bl_pred_2D = bl_pred.reshape(shape)\n",
    "    \n",
    "    return bl_pred_2D\n",
    "\n",
    "def get_ml_pred(X, shape, hazard, scale='36km', model='hist', FRAMEWORK=None, TIMESCALE=None):\n",
    "    if FRAMEWORK and TIMESCALE:\n",
    "        in_path=f'/work/samuel.varga/projects/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}/mlModels/{scale}/'\n",
    "    else:\n",
    "        in_path = f'/work/samuel.varga/projects/2to6_hr_severe_wx/mlModels/{scale}/{hazard}'\n",
    "    \n",
    "    # Load the ML model.\n",
    "    ml_data = joblib.load(join(in_path,f'Varga_all_{model}_{hazard}_{scale}.joblib')) #Change this to own path\n",
    "\n",
    "    # When the ML model is saved by the CalibratedPipelineHyperOptCV package, \n",
    "    # there are additional metadata that is stored with it. \n",
    "    # We want to load the model and the features. \n",
    "    # We want to make sure the X input as the features in correct order. \n",
    "    ml_model = ml_data['model']\n",
    "    features = ml_data['features']\n",
    "\n",
    "    X = X[features]\n",
    "    \n",
    "    ml_pred = ml_model.predict_proba(X)[:,1]\n",
    "    ml_pred_2D = ml_pred.reshape(shape)\n",
    "    \n",
    "    return ml_pred_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20685ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/mflora/SummaryFiles/20210524/0000/wofs_ENS_00_20210525_0000_0000.nc\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "if TIMESCALE=='0to3':\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time}/wofs_ENS_00*')[0] #0000 of the next day\n",
    "else:\n",
    "    indir = glob(f'/work/mflora/SummaryFiles/{date}/{init_time}/wofs_ENS_24*')[0] #0000 of the next day\n",
    "print(indir)\n",
    "#indir = glob(f'/work/samuel.varga/data/2to6_hr_severe_wx/SummaryFiles/{date}/{init_time}/wofs_ENS_24*')[0]\n",
    "\n",
    "ds = xr.load_dataset(indir, decode_times=False)\n",
    "lats = ds['xlat'][::3, ::3]\n",
    "lons = ds['xlon'][::3, ::3]\n",
    "\n",
    "shape = (len(lons), len(lats))\n",
    "\n",
    "central_longitude = ds.attrs['STAND_LON']\n",
    "central_latitude = ds.attrs['CEN_LAT']\n",
    "\n",
    "standard_parallels = (ds.attrs['TRUELAT1'], ds.attrs['TRUELAT2'])\n",
    "projection=ccrs.LambertConformal(central_longitude=central_longitude,\n",
    "                                 central_latitude=central_latitude,\n",
    "                                 standard_parallels=standard_parallels)\n",
    "crs = ccrs.PlateCarree()\n",
    "data_path = '/home/monte.flora/python_packages/WoF_post/wofs/data/'\n",
    "states = NaturalEarthFeature(category=\"cultural\", scale=\"10m\",\n",
    "                             facecolor=\"none\",\n",
    "                             name=\"admin_1_states_provinces\")\n",
    "\n",
    "county_file = join(data_path,'COUNTIES', 'countyl010g')\n",
    "reader = shpreader.Reader(county_file)\n",
    "shape_feature = ShapelyFeature(reader.geometries(),\n",
    "                               crs, facecolor='none', linewidth=0.2, edgecolor='black', )\n",
    "\n",
    "def add_map_stuff(ax, states, shape_feature):\n",
    "    ax.add_feature(states, linewidth=.1, facecolor='none', edgecolor=\"black\")\n",
    "    ax.add_feature(cfeature.LAKES, linewidth=.1, facecolor='none', edgecolor=\"black\")\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=.1, facecolor='none', edgecolor=\"black\")        \n",
    "    ax.add_feature(shape_feature)\n",
    "    \n",
    "    \n",
    "def set_extent(ax, projection , crs, lat, lons,):\n",
    "    \"\"\" Set the Map extent based the WoFS domain \"\"\"\n",
    "    # Set the extent. \n",
    "    xs, ys, _ = projection.transform_points(\n",
    "            crs,\n",
    "            np.array([lons.min(), lons.max()]),\n",
    "            np.array([lats.min(), lats.max()])).T\n",
    "    _xlimits = xs.tolist()\n",
    "    _ylimits = ys.tolist()\n",
    "\n",
    "    # The limit is max(lower bound), min(upper bound). This will create \n",
    "    # a square plot and make sure there is no white spaces between the map\n",
    "    # the bounding box created by matplotlib. This also allows us to set the\n",
    "    # WoFS domain boundaries in cases where we aren't plotting WoFS data \n",
    "    # (e.g., storm reports, warning polygons, etc.) \n",
    "    lims = (max([_xlimits[0]]+[_ylimits[0]]),min([_xlimits[-1]]+[_ylimits[-1]]))\n",
    "        \n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims) \n",
    "    \n",
    "    return ax #0000 is 0000 the next day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6ba9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(StormReportDownloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10891510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the storm reports-- Should add a check to grab data if it isn't in directory\n",
    "#StormReportDownloader('/work/samuel.varga/data/2to6_hr_severe_wx/reports/').get_storm_events(years=[2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e556bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202105250000\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import uniform_filter, maximum_filter \n",
    "#from WoF_post.wofs.verification.lsrs.get_storm_reports import StormReports\n",
    "#from WoF_post.wofs.plotting.util import decompose_file_path\n",
    "#from WoF_post.wofs.plotting.wofs_colors import WoFSColors\n",
    "\n",
    "from wofs.plotting.util import decompose_file_path\n",
    "from wofs.plotting.wofs_colors import WoFSColors\n",
    "#from wofs.verification.lsrs.get_storm_reports import StormReports\n",
    "from wofs.verification.lsrs.get_storm_reports import StormReports\n",
    "\n",
    "# Get the storm reports. \n",
    "comps = decompose_file_path(indir)\n",
    "\n",
    "if TIMESCALE=='0to3':\n",
    "    #init_time = comps['VALID_DATE']+comps['VALID_TIME'] \n",
    "    init_time = comps['VALID_DATE']+comps['INIT_TIME'] #0000 of date+1\n",
    "    #init_time=str(int(comps['VALID_DATE'])+1)+'0000' #0000 of date +1\n",
    "else:\n",
    "    init_time=str(int(comps['VALID_DATE'])+1)+'0000' #0000 of date+1, which is the valid time\n",
    "    \n",
    "print(init_time)\n",
    "\n",
    "\n",
    "#####Need to find Dir where stormreports are located\n",
    "if TIMESCALE=='0to3':\n",
    "    forecast_length=180\n",
    "else:\n",
    "    forecast_length=240\n",
    "    \n",
    "report = StormReportLoader(initial_time=init_time, \n",
    "            forecast_length=forecast_length, \n",
    "            err_window=15, \n",
    "            reports_path='/work/samuel.varga/data/2to6_hr_severe_wx/reports/STORM_EVENTS_{0}-{0}.csv'.format(str(date)[0:4]), #change this\n",
    "            report_type='NOAA'\n",
    "            )\n",
    "\n",
    "ds = xr.load_dataset(indir, decode_times=False)\n",
    "points = report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "7dfea4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202105250000\n",
      "/work/mflora/SummaryFiles/20210524/2200/wofs_ENS_24_20210524_2200_0000.nc\n"
     ]
    }
   ],
   "source": [
    "#help(StormReports)\n",
    "print(init_time)\n",
    "print(indir) #Why is this the next day?  #03: 202005220000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "9d92157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {'hail' : 'Severe Hail', \n",
    "         'wind' : 'Severe Wind', \n",
    "         'tornado' : 'Tornado',\n",
    "         'all': 'All Severe'}\n",
    "\n",
    "baseline_names = {'hail' : 'Calibrated NMEP (45 km) of HAILCAST > 1.0 in',\n",
    "                  'wind' : 'Calibrated NMEP (45 km) of 80-m wind speed > 40 kts',\n",
    "                  'tornado' : r'Calibrated NMEP (45 km) of UH > 150 $m^2 s^{-2}$',\n",
    "                  'all' : r'Calibrated NMEP (27 KM) of UH > 50 $m^2 s^{-2}$'\n",
    "                 } #Need to update the UH threshold title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "d9247bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rprt_df=geo_df.copy()\n",
    "points[hazard].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f09e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "hazard='all'\n",
    "\n",
    "\n",
    "hazard_color = {'hail' : 'g', 'wind': 'b', 'tornado': 'r'}\n",
    "fig, axes = plt.subplots(figsize=(10,8), facecolor='w',\n",
    "                     dpi=170, nrows=2, ncols=2, subplot_kw={'projection': projection}, \n",
    "                        constrained_layout=True)\n",
    "\n",
    "models = ['BL', 'hist', 'logistic', 'random'] #random/ADAM\n",
    "model_names = [baseline_names[hazard], 'GBT', 'LR', 'ADAM RF'] #RF/ADAM\n",
    "\n",
    "if hazard != 'tornado':\n",
    "    levels = np.arange(0, 1.1, 0.1)\n",
    "else:\n",
    "    levels = np.arange(0, 0.5, 0.001)  \n",
    "    \n",
    "# All 3 Models and BL\n",
    "for ax, model, name in zip(axes.flat, models, model_names):\n",
    "    if model == 'BL':\n",
    "        pred = get_bl_pred(X_bl, shape, hazard, scale, FRAMEWORK=FRAMEWORK, TIMESCALE=TIMESCALE)\n",
    "    else:\n",
    "        pred = get_ml_pred(X, shape, hazard, scale, model, FRAMEWORK=FRAMEWORK, TIMESCALE=TIMESCALE)\n",
    "    \n",
    "    add_map_stuff(ax, states, shape_feature)\n",
    "    pred = np.ma.masked_where(pred<=0.025, pred)\n",
    "    \n",
    "    #cmap=WoFSColors.wz_cmap\n",
    "    cf = ax.contourf(lons,lats, pred, cmap=WoFSColors.wz_cmap, alpha=0.95, levels=levels,\n",
    "                transform = crs, )\n",
    "\n",
    "    ##point_lat, point_lon = points[hazard]\n",
    "    if All:\n",
    "        for Lorem in ['hail','wind','tornado']:\n",
    "            _points=points[Lorem]\n",
    "            ax.scatter(_points[:,1],_points[:,0], s=10, color=hazard_color[Lorem], alpha=0.8, zorder=1, transform=crs)\n",
    "    else:\n",
    "        _points = points[hazard]\n",
    "        ax.scatter(_points[:,1],_points[:,0], s=10, color='k', alpha=0.8, zorder=1, transform=crs)\n",
    "    \n",
    "    \n",
    "    ax = set_extent(ax, projection , crs, lats, lons,)\n",
    "    fontsize = 10 if len(name) > 5 else 12\n",
    "    ax.set_title(name, fontsize=fontsize)\n",
    "\n",
    "fig.tight_layout()\n",
    "#plt.subplots_adjust(hspace=0.2, wspace=0.0)\n",
    "axes[0,1].annotate(title, (0.3, 1.1), xycoords='axes fraction', fontsize=10, color='k')\n",
    "\n",
    "cax = fig.add_axes([0.25, -0.05, 0.5, 0.05])\n",
    "\n",
    "fig.colorbar(cf, \n",
    "           cax=cax, \n",
    "           label=f'Probability of {titles[hazard]}\\n within {scale} of a point\\nin the next {TIMESCALE} Hours', #shouldn't this be 36km?\n",
    "          orientation='horizontal')\n",
    "\n",
    "\n",
    "#Add 36/18/9km radius to storm reports\n",
    "if False:\n",
    "    for ax in axes.flat:  \n",
    "        if All:\n",
    "            for Lorem in ['hail','wind','tornado']:\n",
    "                _points = points[Lorem]\n",
    "                for pair in zip(_points[:,1], _points[:,0]):\n",
    "                    repX, repY = circleofRadius(pair[1],pair[0], 36)\n",
    "                    ax.plot(repX, repY, transform=crs, color='k', linewidth=1)\n",
    "        else:\n",
    "            for pair in zip(_points[:,1], _points[:,0]):\n",
    "                repX, repY = circleofRadius(pair[1],pair[0], 36)\n",
    "                ax.plot(repX, repY, transform=crs, color='k', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "1f7118d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circleofRadius(lat, lon, radiuskm):\n",
    "    latArray=[]\n",
    "    lonArray=[]\n",
    "    for brng in range(0,360):\n",
    "        lat2, lon2 = getLocation(lat,lon,brng, radiuskm)\n",
    "        latArray.append(lat2)\n",
    "        lonArray.append(lon2)\n",
    "    return lonArray, latArray\n",
    "        \n",
    "def getLocation(lat1, lon1, brng, radiuskm):\n",
    "    lat1=lat1*np.pi/180.0\n",
    "    lon1=lon1*np.pi/180.0\n",
    "    R=6378.1 #radius of earth in km\n",
    "    distancekm=radiuskm/R\n",
    "    brng=(brng/90)*np.pi/2\n",
    "    lat2=np.arcsin(np.sin(lat1)*np.cos(distancekm)+np.cos(lat1)*np.sin(distancekm)*np.cos(brng))\n",
    "    lon2=lon1+np.arctan2(np.sin(brng)*np.sin(distancekm)*np.cos(lat1), np.cos(distancekm)-np.sin(lat1)*np.sin(lat2))\n",
    "    lon2 = 180.0 * lon2/np.pi\n",
    "    lat2 = 180.0*lat2/np.pi\n",
    "    \n",
    "    return lat2, lon2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099cf99",
   "metadata": {},
   "source": [
    "I wonder if there is some sort of autocorrelation in the storm reports, will the models be more likely to forecast high probabilities for regions with higher populations bc they're more likely to have a storm report? - It shouldn't be a major issue. The predictions are based on Met. fields; as long as the conditions are correct, it will predict severe wx regardless of population. That's moreso a data issue, where we may lose some predictability due to storms in less populated regions not being associated with a storm report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c04dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
