{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c90a357",
   "metadata": {},
   "source": [
    "## Training the Machine Learning Models \n",
    "\n",
    "### Primary Goal: Train an accurate machine model for the individual severe weather hazards. \n",
    "### Second Goal: Develop the models such that they outperform the baseline models. \n",
    "\n",
    "In this notebook, I'll provide a brief tutorial on how to train and evaluate a baseline model. It is not only helpful, but crucial to develop a simplier, baseline model against which to evaluate the skill of the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeae7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting code imports \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/monte.flora/python_packages/2to6_hr_severe_wx/')\n",
    "from main.io import load_ml_data\n",
    "\n",
    "from master.ml_workflow.ml_workflow.calibrated_pipeline_hyperopt_cv import CalibratedPipelineHyperOptCV\n",
    "\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from ml_workflow.ml_workflow.ml_methods import norm_aupdc, brier_skill_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from hyperopt import hp\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca35711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables (You'll need to change based on where you store your data)\n",
    "base_path = '/work/mflora/ML_2TO6HR/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a01c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='train', \n",
    "                            target_col='hail_severe__36km')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b567732",
   "metadata": {},
   "source": [
    "## CalibratedPipelineHyperOptCV\n",
    "\n",
    "This is a scikit-learn style model I've developed. As the name suggests, it handles the following: \n",
    "#### 1. Creating an ML pipeline. \n",
    "Often data requires pre-process such as imputing missing values, scaling the data, or re-sampling the data to fix class imbalances prior to fitting the ML model. CalibratedPipelineHyperOptCV has 3 options for the pipeline : `scaler`, `imputer`, and `resample`. You'll like keep the options at `scaler = 'standard'`, `imputer='simple'`, and `resample='under'` for the REU, but feel free to explore different options! \n",
    " \n",
    "#### 2. Performing Hyperparameter Optimization \n",
    "Many machine learning models have tunable knobs. For example, in random forest we can set the number of trees. How do you know we have chosen the best-performing options? The simplest, but most time-consuming option is brute force where we try different hyperparameters and evaluate the performance on a validation dataset. CalibratedPipelineHyperOptCV uses the [hyperopt](http://hyperopt.github.io/hyperopt/) package and cross-validation to determine the best-performing hyperparameters. For the hyperparameter optimization, you set the number of iterations (`max_iter`). \n",
    "\n",
    "To use the hyperparameter optimization, you'll need to set a parameter grid (`param_grid`). An example grid for logistic regression is provided below. Feel free to ask questions about setting up a grid for other models. \n",
    "\n",
    "\n",
    "#### 3. Calibration \n",
    "Machine learning models, especially those trained on resampled data, tend to be uncalibrated. We perform cross-validation-based calibration using isotonic regression. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d16ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this and run it to see the different CalibratedPipelineHyperOptCV options. \n",
    "#help(CalibratedPipelineHyperOptCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c4a57",
   "metadata": {},
   "source": [
    "## Example. \n",
    "\n",
    "This example shows how to train a simple logistic regression with elastic nets using the CalibratedPipelineHyperOptCV package. Since logistic regression requires the different features to have similar scales, we set `scaler = 'standard'`. There is significant class imbalance (most of the examples are non-severe), so we have `resample = 'under'`. \n",
    "\n",
    "For the cross-validation argument (`cv_kwargs`), we want to pass in the training dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba47a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                               | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/dask/dataframe/backends.py:187: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▊                                                                                                                                                                               | 1/100 [09:43<16:03:12, 583.76s/trial, best loss: 0.8459230868377026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n",
      "/home/monte.flora/miniconda3/envs/wofs_test/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = 'standard'\n",
    "resample = 'under'\n",
    "\n",
    "base_estimator = LogisticRegression(solver='saga', penalty='elasticnet', max_iter=300, random_state=42)\n",
    "param_grid = {\n",
    "                'l1_ratio': hp.choice('l1_ratio', [0.0001, 0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1.0]),\n",
    "                'C': hp.choice('C', [0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]),\n",
    "                }\n",
    " \n",
    "train_dates = metadata['Run Date'].apply(str)\n",
    "\n",
    "clf = CalibratedPipelineHyperOptCV(base_estimator=base_estimator, \n",
    "                                   param_grid=param_grid, \n",
    "                                   scaler=scaler, \n",
    "                                   resample=resample, max_iter=10, \n",
    "                                   cv_kwargs = {'dates': train_dates, 'n_splits': 5, 'valid_size' : 20} )\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "save_name = 'hail_model.joblib'\n",
    "clf.save(save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
