{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea542d3",
   "metadata": {},
   "source": [
    "## Training the Machine Learning Models \n",
    "\n",
    "### Primary Goal: Train an accurate machine model for the individual severe weather hazards. \n",
    "### Second Goal: Develop the models such that they outperform the baseline models. \n",
    "\n",
    "In this notebook, I'll provide a brief tutorial on how to train and evaluate a baseline model. It is not only helpful, but crucial to develop a simplier, baseline model against which to evaluate the skill of the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c751e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Always keep this import at the top of your script. It is uses the Intel extension \n",
    "# for scikit-learn, which improves the training speed of machine learning algorithms\n",
    "# in scikit-learn. \n",
    "from ml_workflow.calibrated_pipeline_hyperopt_cv import CalibratedPipelineHyperOptCV\n",
    "\n",
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/monte.flora/python_packages/2to6_hr_severe_wx/')\n",
    "from main.io import load_ml_data\n",
    "\n",
    "from os.path import join\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb37016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables (You'll need to change based on where you store your data)\n",
    "base_path = '/work/mflora/ML_2TO6HR/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f26ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='train', \n",
    "                            target_col='hail_severe__36km')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a42f8e",
   "metadata": {},
   "source": [
    "## CalibratedPipelineHyperOptCV\n",
    "\n",
    "This is a scikit-learn style model I've developed. As the name suggests, it handles the following: \n",
    "#### 1. Creating an ML pipeline. \n",
    "Often data requires pre-process such as imputing missing values, scaling the data, or re-sampling the data to fix class imbalances prior to fitting the ML model. CalibratedPipelineHyperOptCV has 3 options for the pipeline : `scaler`, `imputer`, and `resample`. You'll like keep the options at `scaler = 'standard'`, `imputer='simple'`, and `resample='under'` for the REU, but feel free to explore different options! \n",
    " \n",
    "#### 2. Performing Hyperparameter Optimization \n",
    "Many machine learning models have tunable knobs. For example, in random forest we can set the number of trees. How do you know we have chosen the best-performing options? The simplest, but most time-consuming option is brute force where we try different hyperparameters and evaluate the performance on a validation dataset. CalibratedPipelineHyperOptCV uses the [hyperopt](http://hyperopt.github.io/hyperopt/) package and cross-validation to determine the best-performing hyperparameters. For the hyperparameter optimization, you set the number of iterations (`max_iter`). \n",
    "\n",
    "To use the hyperparameter optimization, you'll need to set a parameter grid (`param_grid`). An example grid for logistic regression is provided below. Feel free to ask questions about setting up a grid for other models. \n",
    "\n",
    "\n",
    "#### 3. Calibration \n",
    "Machine learning models, especially those trained on resampled data, tend to be uncalibrated. We perform cross-validation-based calibration using isotonic regression. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ddb747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this and run it to see the different CalibratedPipelineHyperOptCV options. \n",
    "#help(CalibratedPipelineHyperOptCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0fb304",
   "metadata": {},
   "source": [
    "## Example. \n",
    "\n",
    "This example shows how to train a simple logistic regression with elastic nets using the CalibratedPipelineHyperOptCV package. Since logistic regression requires the different features to have similar scales, we set `scaler = 'standard'`. There is significant class imbalance (most of the examples are non-severe), so we have `resample = 'under'`. \n",
    "\n",
    "For the cross-validation argument (`cv_kwargs`), we want to pass in the training dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78942a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|██                                                                                                                                                                                                        | 1/100 [04:11<6:54:56, 251.48s/trial, best loss: 0.8450426915932777]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monte.flora/miniconda3/envs/reu/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = 'standard'\n",
    "resample = 'under'\n",
    "\n",
    "base_estimator = LogisticRegression(solver='saga', penalty='elasticnet', max_iter=300, random_state=42)\n",
    "param_grid = {\n",
    "                'l1_ratio': hp.choice('l1_ratio', [0.0001, 0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1.0]),\n",
    "                'C': hp.choice('C', [0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]),\n",
    "                }\n",
    " \n",
    "train_dates = metadata['Run Date'].apply(str)\n",
    "\n",
    "clf = CalibratedPipelineHyperOptCV(base_estimator=base_estimator, \n",
    "                                   param_grid=param_grid, \n",
    "                                   scaler=scaler, \n",
    "                                   resample=resample, max_iter=10, \n",
    "                                   cv_kwargs = {'dates': train_dates, 'n_splits': 5, 'valid_size' : 20} )\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "save_name = 'hail_model.joblib'\n",
    "clf.save(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62ec22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
