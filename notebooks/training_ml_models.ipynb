{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea542d3",
   "metadata": {},
   "source": [
    "## Training the Machine Learning Models \n",
    "\n",
    "### Primary Goal: Train an accurate machine model for the individual severe weather hazards. \n",
    "### Second Goal: Develop the models such that they outperform the baseline models. \n",
    "\n",
    "In this notebook, I'll provide a brief tutorial on how to train and evaluate a macine learning model. It is not only helpful, but crucial to develop a simplier, baseline model against which to evaluate the skill of the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c751e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Always keep this import at the top of your script. It is uses the Intel extension \n",
    "# for scikit-learn, which improves the training speed of machine learning algorithms\n",
    "# in scikit-learn. \n",
    "\n",
    "# We add the github package to our system path so we can import python scripts for that repo. \n",
    "import sys\n",
    "sys.path.append('/home/samuel.varga/projects/2to6_hr_severe_wx/')\n",
    "sys.path.append('/home/samuel.varga/python_packages/ml_workflow/')\n",
    "from main.io import load_ml_data\n",
    "from ml_workflow.calibrated_pipeline_hyperopt_cv import CalibratedPipelineHyperOptCV\n",
    "\n",
    "# Import packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from os.path import join\n",
    "from os.path import join\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb37016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables (You'll need to change based on where you store your data)\n",
    "FRAMEWORK='POTVIN'\n",
    "TIMESCALE='0to3'\n",
    "base_path = f'/work/samuel.varga/data/{TIMESCALE}_hr_severe_wx/{FRAMEWORK}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f26ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139737\n",
      "243973\n",
      "255611\n"
     ]
    }
   ],
   "source": [
    "target_col='wind_severe__36km'\n",
    "scale='36'\n",
    "All=True\n",
    "if All: #Use all severe as target\n",
    "    X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='train', \n",
    "                            target_col=target_col,\n",
    "                           FRAMEWORK=FRAMEWORK,\n",
    "                           TIMESCALE=TIMESCALE)\n",
    "    print(len(y[y>0]))\n",
    "    for hazard in ['hail','tornado']:\n",
    "        target_col='{}_severe__{}km'.format(hazard, scale)\n",
    "        SPAM, y1, SPAM  = load_ml_data(base_path=base_path, mode='train', target_col=target_col, FRAMEWORK=FRAMEWORK, TIMESCALE=TIMESCALE) \n",
    "        y +=y1\n",
    "        print(len(y[y>0]))\n",
    "       \n",
    "    y[y > 0] = 1\n",
    "else:\n",
    "    X,y,metadata = load_ml_data(base_path=base_path, \n",
    "                            mode='train', \n",
    "                            target_col=target_col)\n",
    "#list(X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a42f8e",
   "metadata": {},
   "source": [
    "## CalibratedPipelineHyperOptCV\n",
    "\n",
    "This is a scikit-learn style model I've developed. As the name suggests, it handles the following: \n",
    "#### 1. Creating an ML pipeline. \n",
    "Often data requires pre-process such as imputing missing values, scaling the data, or re-sampling the data to fix class imbalances prior to fitting the ML model. CalibratedPipelineHyperOptCV has 3 options for the pipeline : `scaler`, `imputer`, and `resample`. You'll likely keep the options at `scaler = 'standard'`, `imputer='simple'`, and `resample='under'` for the REU, but feel free to explore different options! \n",
    " \n",
    "#### 2. Performing Hyperparameter Optimization \n",
    "Many machine learning models have tunable knobs. For example, in random forest we can set the number of trees. How do you know we have chosen the best-performing options? The simplest, but most time-consuming option is brute force where we try different hyperparameters and evaluate the performance on a validation dataset. CalibratedPipelineHyperOptCV uses the [hyperopt](http://hyperopt.github.io/hyperopt/) package and cross-validation to determine the best-performing hyperparameters. For the hyperparameter optimization, you set the number of iterations (`max_iter`). \n",
    "\n",
    "To use the hyperparameter optimization, you'll need to set a parameter grid (`param_grid`). An example grid for logistic regression is provided below. Feel free to ask questions about setting up a grid for other models. \n",
    "\n",
    "\n",
    "#### 3. Calibration \n",
    "Machine learning models, especially those trained on resampled data, tend to be uncalibrated. We perform cross-validation-based calibration using isotonic regression. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ddb747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CalibratedPipelineHyperOptCV in module ml_workflow.calibrated_pipeline_hyperopt_cv:\n",
      "\n",
      "class CalibratedPipelineHyperOptCV(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin, sklearn.base.MetaEstimatorMixin)\n",
      " |  CalibratedPipelineHyperOptCV(base_estimator, param_grid, cal_method='isotonic', imputer='simple', scaler=None, resample=None, local_dir='/home/samuel.varga/projects/2to6_hr_severe_wx/notebooks', n_jobs=1, max_iter=15, scorer=<function norm_csi_scorer at 0x7f5acc893eb0>, cv='kfolds', cv_kwargs={'n_splits': 5}, hyperopt='atpe', scorer_kwargs={})\n",
      " |  \n",
      " |  This class takes X,y as inputs and returns \n",
      " |  a ML pipeline with optimized hyperparameters (through k-folds cross-validation)  \n",
      " |  that is also calibrated using isotonic regression. \n",
      " |  \n",
      " |  Parameters:\n",
      " |  ---------------------------\n",
      " |      base_estimator : unfit callable classifier or regressor (likely from scikit-learn) \n",
      " |                  that implements a ``fit`` method. \n",
      " |      \n",
      " |      param_grid : dict of hyperparameters and the grid to search over. \n",
      " |      \n",
      " |      E.g., param for sklearn.ensemble.RandomForestClassiferm could be \n",
      " |      \n",
      " |          param_grid = {  'n_estimators' : [100,150,300,400,500], \n",
      " |              'max_depth' : [6,8,10,15,20],\n",
      " |              'max_features' : [5,6,8,10],\n",
      " |              'min_samples_split' : [4,5,8,10,15,20,25,50],\n",
      " |              'min_samples_leaf' : [4,5,8,10,15,20,25,50],\n",
      " |           }\n",
      " |      For simplicity, this method only implements choices and not ranges. So the\n",
      " |      ranges must be defined as lists. \n",
      " |      \n",
      " |      \n",
      " |      scorer : callable object/function that takes inputs (model,X,y, **scorer_kwargs).\n",
      " |               Used as the scorer for the hyperopt package to optimize hyperparameters, \n",
      " |               so the scorer must return a loss metric (where a lower value is better).\n",
      " |               If the score is rank-based (like AUC), then have scorer\n",
      " |               return 1 - AUC. \n",
      " |                \n",
      " |      scorer_kwargs : Any keyword arguments to pass into scorer. \n",
      " |      \n",
      " |      imputer : ``simple``,  ``iterative``, or None. If not None, then an imputer is included \n",
      " |                 in the pipeline. Imputers are used to replace missing values. \n",
      " |                 \n",
      " |                 - ``simple`` uses sklearn.impute.SimpleImputer\n",
      " |                 - ``iterative`` uses sklearn.impute.IterativeImputer\n",
      " |                 \n",
      " |      scaler : ``standard``,  ``robust``, or None. \n",
      " |              If not None, then \n",
      " |              the dataset is scaled (useful for regression models when the \n",
      " |              scale of the predictors varies)\n",
      " |              \n",
      " |              - ``standard`` uses sklearn.preprocessing.StandardScaler\n",
      " |              - ``robust`` uses sklearn.preprocessing.RobustScaler\n",
      " |      \n",
      " |      resample : ``under``, ``over``, or None. \n",
      " |                  If not None, then the dataset\n",
      " |                 is resampled to balance the dataset. \n",
      " |                 \n",
      " |              - ``under`` uses imblearn.under_sampling.RandomUnderSampler\n",
      " |              - ``over`` uses imblearn.over_sampling.RandomOverSampler\n",
      " |                 \n",
      " |      local_dir : str/path or None \n",
      " |              Filename to save the results of the hyperparameter optimization.  \n",
      " |              Directory where the results of the hyperparameter optimization are stored.\n",
      " |              Typically, ~/hyperopt_results.pkl\n",
      " |      \n",
      " |      max_iter : int \n",
      " |          Number of iterations for early stopping in the hyperopt. The system will check whether the \n",
      " |          performance has improved within max_iter, otherwise the process stops. \n",
      " |      \n",
      " |      n_jobs : int (default=1)\n",
      " |          Number of processors for processing the cross-validation in parallel\n",
      " |          \n",
      " |      hyperopt : ``atpe`` or ``tpe``\n",
      " |              Options for the hyperopt python package. This is the\n",
      " |              optimization algorithm used. \n",
      " |  \n",
      " |              - ``tpe`` Tree of Parzen Estimators (TPE)\n",
      " |              - ``atpe`` Adaptive TPE\n",
      " |      \n",
      " |      cv : ``date_based`` or ``kfold``. \n",
      " |              \n",
      " |      cv_kwargs : dict \n",
      " |          - n_splits : int\n",
      " |              Number of cross-validation folds \n",
      " |          \n",
      " |          - dates,  shape = (n_samples,)\n",
      " |              Used for date-based cross-validation generation \n",
      " |              that recognize the autocorrelation within meteorological data. This column could\n",
      " |              days, month, etc. shape = (n_samples,)\n",
      " |          \n",
      " |          - valid_size : int or float between 0 and 1\n",
      " |              If int, then interpreted as the number of dates to include \n",
      " |              in each validation dataset. E,g, if valid_size=5 and \n",
      " |              dates is months, then each validation dataset will \n",
      " |              include 5 months \n",
      " |          \n",
      " |          \n",
      " |  Attributes:\n",
      " |  --------------------\n",
      " |      estimator_ : a fit classifier model\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CalibratedPipelineHyperOptCV\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, base_estimator, param_grid, cal_method='isotonic', imputer='simple', scaler=None, resample=None, local_dir='/home/samuel.varga/projects/2to6_hr_severe_wx/notebooks', n_jobs=1, max_iter=15, scorer=<function norm_csi_scorer at 0x7f5acc893eb0>, cv='kfolds', cv_kwargs={'n_splits': 5}, hyperopt='atpe', scorer_kwargs={})\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bulid_pipeline(self)\n",
      " |  \n",
      " |  convert_tuning_results(self, fname)\n",
      " |      Convert the hyperopt results to pickle file\n",
      " |  \n",
      " |  fit(self, X, y, params=None)\n",
      " |      Fit the estimator \n",
      " |      \n",
      " |      Parameters:\n",
      " |      ----------------\n",
      " |          X, pd.DataFrame, shape : (n_samples, n_features)\n",
      " |          y, shape: (n_samples, )\n",
      " |  \n",
      " |  init_cv(self)\n",
      " |      Initialize the cross-validation generator\n",
      " |  \n",
      " |  objective(self, params)\n",
      " |      Objective function for Hyperparameter Optimization\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the target of new samples. Can be different from the\n",
      " |      prediction of the uncalibrated classifier.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          The predicted class.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Posterior probabilities of classification\n",
      " |      \n",
      " |      This function returns posterior probabilities of classification\n",
      " |      according to each class on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples, n_classes)\n",
      " |          The predicted probas.\n",
      " |  \n",
      " |  save(self, fname)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this and run it to see the different CalibratedPipelineHyperOptCV options. \n",
    "help(CalibratedPipelineHyperOptCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0fb304",
   "metadata": {},
   "source": [
    "## Example. \n",
    "\n",
    "This example shows how to train a simple logistic regression with elastic nets using the CalibratedPipelineHyperOptCV package. Since logistic regression requires the different features to have similar scales, we set `scaler = 'standard'`. There is significant class imbalance (most of the examples are non-severe), so we have `resample = 'under'`. \n",
    "\n",
    "For the cross-validation argument (`cv_kwargs`), we want to pass in the training dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262e07c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uh_2to5_instant__time_max__45km__ens_90th',\n",
       " 'uh_0to2_instant__time_max__45km__ens_90th',\n",
       " 'wz_0to2_instant__time_max__45km__ens_90th',\n",
       " 'comp_dz__time_max__45km__ens_90th',\n",
       " 'ws_80__time_max__45km__ens_90th',\n",
       " 'hailcast__time_max__45km__ens_90th',\n",
       " 'w_up__time_max__45km__ens_90th',\n",
       " 'okubo_weiss__time_max__45km__ens_90th',\n",
       " 'uh_2to5_instant__time_max__45km__ens_mean',\n",
       " 'uh_0to2_instant__time_max__45km__ens_mean',\n",
       " 'wz_0to2_instant__time_max__45km__ens_mean',\n",
       " 'comp_dz__time_max__45km__ens_mean',\n",
       " 'ws_80__time_max__45km__ens_mean',\n",
       " 'hailcast__time_max__45km__ens_mean',\n",
       " 'w_up__time_max__45km__ens_mean',\n",
       " 'okubo_weiss__time_max__45km__ens_mean',\n",
       " 'uh_2to5_instant__time_max__45km__ens_2nd',\n",
       " 'uh_0to2_instant__time_max__45km__ens_2nd',\n",
       " 'wz_0to2_instant__time_max__45km__ens_2nd',\n",
       " 'comp_dz__time_max__45km__ens_2nd',\n",
       " 'ws_80__time_max__45km__ens_2nd',\n",
       " 'hailcast__time_max__45km__ens_2nd',\n",
       " 'w_up__time_max__45km__ens_2nd',\n",
       " 'okubo_weiss__time_max__45km__ens_2nd',\n",
       " 'uh_2to5_instant__time_max__45km__ens_IQR',\n",
       " 'uh_0to2_instant__time_max__45km__ens_IQR',\n",
       " 'wz_0to2_instant__time_max__45km__ens_IQR',\n",
       " 'comp_dz__time_max__45km__ens_IQR',\n",
       " 'ws_80__time_max__45km__ens_IQR',\n",
       " 'hailcast__time_max__45km__ens_IQR',\n",
       " 'w_up__time_max__45km__ens_IQR',\n",
       " 'okubo_weiss__time_max__45km__ens_IQR',\n",
       " 'uh_2to5_instant__time_max__45km__ens_16th',\n",
       " 'uh_0to2_instant__time_max__45km__ens_16th',\n",
       " 'wz_0to2_instant__time_max__45km__ens_16th',\n",
       " 'comp_dz__time_max__45km__ens_16th',\n",
       " 'ws_80__time_max__45km__ens_16th',\n",
       " 'hailcast__time_max__45km__ens_16th',\n",
       " 'w_up__time_max__45km__ens_16th',\n",
       " 'okubo_weiss__time_max__45km__ens_16th',\n",
       " 'uh_2to5_instant__time_max__27km__ens_90th',\n",
       " 'uh_0to2_instant__time_max__27km__ens_90th',\n",
       " 'wz_0to2_instant__time_max__27km__ens_90th',\n",
       " 'comp_dz__time_max__27km__ens_90th',\n",
       " 'ws_80__time_max__27km__ens_90th',\n",
       " 'hailcast__time_max__27km__ens_90th',\n",
       " 'w_up__time_max__27km__ens_90th',\n",
       " 'okubo_weiss__time_max__27km__ens_90th',\n",
       " 'uh_2to5_instant__time_max__27km__ens_mean',\n",
       " 'uh_0to2_instant__time_max__27km__ens_mean',\n",
       " 'wz_0to2_instant__time_max__27km__ens_mean',\n",
       " 'comp_dz__time_max__27km__ens_mean',\n",
       " 'ws_80__time_max__27km__ens_mean',\n",
       " 'hailcast__time_max__27km__ens_mean',\n",
       " 'w_up__time_max__27km__ens_mean',\n",
       " 'okubo_weiss__time_max__27km__ens_mean',\n",
       " 'uh_2to5_instant__time_max__27km__ens_2nd',\n",
       " 'uh_0to2_instant__time_max__27km__ens_2nd',\n",
       " 'wz_0to2_instant__time_max__27km__ens_2nd',\n",
       " 'comp_dz__time_max__27km__ens_2nd',\n",
       " 'ws_80__time_max__27km__ens_2nd',\n",
       " 'hailcast__time_max__27km__ens_2nd',\n",
       " 'w_up__time_max__27km__ens_2nd',\n",
       " 'okubo_weiss__time_max__27km__ens_2nd',\n",
       " 'uh_2to5_instant__time_max__27km__ens_IQR',\n",
       " 'uh_0to2_instant__time_max__27km__ens_IQR',\n",
       " 'wz_0to2_instant__time_max__27km__ens_IQR',\n",
       " 'comp_dz__time_max__27km__ens_IQR',\n",
       " 'ws_80__time_max__27km__ens_IQR',\n",
       " 'hailcast__time_max__27km__ens_IQR',\n",
       " 'w_up__time_max__27km__ens_IQR',\n",
       " 'okubo_weiss__time_max__27km__ens_IQR',\n",
       " 'uh_2to5_instant__time_max__27km__ens_16th',\n",
       " 'uh_0to2_instant__time_max__27km__ens_16th',\n",
       " 'wz_0to2_instant__time_max__27km__ens_16th',\n",
       " 'comp_dz__time_max__27km__ens_16th',\n",
       " 'ws_80__time_max__27km__ens_16th',\n",
       " 'hailcast__time_max__27km__ens_16th',\n",
       " 'w_up__time_max__27km__ens_16th',\n",
       " 'okubo_weiss__time_max__27km__ens_16th',\n",
       " 'uh_2to5_instant__time_max__9km__ens_90th',\n",
       " 'uh_0to2_instant__time_max__9km__ens_90th',\n",
       " 'wz_0to2_instant__time_max__9km__ens_90th',\n",
       " 'comp_dz__time_max__9km__ens_90th',\n",
       " 'ws_80__time_max__9km__ens_90th',\n",
       " 'hailcast__time_max__9km__ens_90th',\n",
       " 'w_up__time_max__9km__ens_90th',\n",
       " 'okubo_weiss__time_max__9km__ens_90th',\n",
       " 'uh_2to5_instant__time_max__9km__ens_mean',\n",
       " 'uh_0to2_instant__time_max__9km__ens_mean',\n",
       " 'wz_0to2_instant__time_max__9km__ens_mean',\n",
       " 'comp_dz__time_max__9km__ens_mean',\n",
       " 'ws_80__time_max__9km__ens_mean',\n",
       " 'hailcast__time_max__9km__ens_mean',\n",
       " 'w_up__time_max__9km__ens_mean',\n",
       " 'okubo_weiss__time_max__9km__ens_mean',\n",
       " 'uh_2to5_instant__time_max__9km__ens_2nd',\n",
       " 'uh_0to2_instant__time_max__9km__ens_2nd',\n",
       " 'wz_0to2_instant__time_max__9km__ens_2nd',\n",
       " 'comp_dz__time_max__9km__ens_2nd',\n",
       " 'ws_80__time_max__9km__ens_2nd',\n",
       " 'hailcast__time_max__9km__ens_2nd',\n",
       " 'w_up__time_max__9km__ens_2nd',\n",
       " 'okubo_weiss__time_max__9km__ens_2nd',\n",
       " 'uh_2to5_instant__time_max__9km__ens_IQR',\n",
       " 'uh_0to2_instant__time_max__9km__ens_IQR',\n",
       " 'wz_0to2_instant__time_max__9km__ens_IQR',\n",
       " 'comp_dz__time_max__9km__ens_IQR',\n",
       " 'ws_80__time_max__9km__ens_IQR',\n",
       " 'hailcast__time_max__9km__ens_IQR',\n",
       " 'w_up__time_max__9km__ens_IQR',\n",
       " 'okubo_weiss__time_max__9km__ens_IQR',\n",
       " 'uh_2to5_instant__time_max__9km__ens_16th',\n",
       " 'uh_0to2_instant__time_max__9km__ens_16th',\n",
       " 'wz_0to2_instant__time_max__9km__ens_16th',\n",
       " 'comp_dz__time_max__9km__ens_16th',\n",
       " 'ws_80__time_max__9km__ens_16th',\n",
       " 'hailcast__time_max__9km__ens_16th',\n",
       " 'w_up__time_max__9km__ens_16th',\n",
       " 'okubo_weiss__time_max__9km__ens_16th',\n",
       " 'mid_level_lapse_rate__time_avg__45km__ens_mean',\n",
       " 'low_level_lapse_rate__time_avg__45km__ens_mean',\n",
       " 'shear_u_0to1__time_avg__45km__ens_mean',\n",
       " 'shear_v_0to1__time_avg__45km__ens_mean',\n",
       " 'shear_u_0to6__time_avg__45km__ens_mean',\n",
       " 'shear_v_0to6__time_avg__45km__ens_mean',\n",
       " 'shear_u_3to6__time_avg__45km__ens_mean',\n",
       " 'shear_v_3to6__time_avg__45km__ens_mean',\n",
       " 'srh_0to3__time_avg__45km__ens_mean',\n",
       " 'cape_ml__time_avg__45km__ens_mean',\n",
       " 'cin_ml__time_avg__45km__ens_mean',\n",
       " 'stp__time_avg__45km__ens_mean',\n",
       " 'scp__time_avg__45km__ens_mean',\n",
       " 'mid_level_lapse_rate__time_avg__45km__ens_std',\n",
       " 'low_level_lapse_rate__time_avg__45km__ens_std',\n",
       " 'shear_u_0to1__time_avg__45km__ens_std',\n",
       " 'shear_v_0to1__time_avg__45km__ens_std',\n",
       " 'shear_u_0to6__time_avg__45km__ens_std',\n",
       " 'shear_v_0to6__time_avg__45km__ens_std',\n",
       " 'shear_u_3to6__time_avg__45km__ens_std',\n",
       " 'shear_v_3to6__time_avg__45km__ens_std',\n",
       " 'srh_0to3__time_avg__45km__ens_std',\n",
       " 'cape_ml__time_avg__45km__ens_std',\n",
       " 'cin_ml__time_avg__45km__ens_std',\n",
       " 'stp__time_avg__45km__ens_std',\n",
       " 'scp__time_avg__45km__ens_std',\n",
       " 'mid_level_lapse_rate__time_avg__27km__ens_mean',\n",
       " 'low_level_lapse_rate__time_avg__27km__ens_mean',\n",
       " 'shear_u_0to1__time_avg__27km__ens_mean',\n",
       " 'shear_v_0to1__time_avg__27km__ens_mean',\n",
       " 'shear_u_0to6__time_avg__27km__ens_mean',\n",
       " 'shear_v_0to6__time_avg__27km__ens_mean',\n",
       " 'shear_u_3to6__time_avg__27km__ens_mean',\n",
       " 'shear_v_3to6__time_avg__27km__ens_mean',\n",
       " 'srh_0to3__time_avg__27km__ens_mean',\n",
       " 'cape_ml__time_avg__27km__ens_mean',\n",
       " 'cin_ml__time_avg__27km__ens_mean',\n",
       " 'stp__time_avg__27km__ens_mean',\n",
       " 'scp__time_avg__27km__ens_mean',\n",
       " 'mid_level_lapse_rate__time_avg__27km__ens_std',\n",
       " 'low_level_lapse_rate__time_avg__27km__ens_std',\n",
       " 'shear_u_0to1__time_avg__27km__ens_std',\n",
       " 'shear_v_0to1__time_avg__27km__ens_std',\n",
       " 'shear_u_0to6__time_avg__27km__ens_std',\n",
       " 'shear_v_0to6__time_avg__27km__ens_std',\n",
       " 'shear_u_3to6__time_avg__27km__ens_std',\n",
       " 'shear_v_3to6__time_avg__27km__ens_std',\n",
       " 'srh_0to3__time_avg__27km__ens_std',\n",
       " 'cape_ml__time_avg__27km__ens_std',\n",
       " 'cin_ml__time_avg__27km__ens_std',\n",
       " 'stp__time_avg__27km__ens_std',\n",
       " 'scp__time_avg__27km__ens_std',\n",
       " 'mid_level_lapse_rate__time_avg__9km__ens_mean',\n",
       " 'low_level_lapse_rate__time_avg__9km__ens_mean',\n",
       " 'shear_u_0to1__time_avg__9km__ens_mean',\n",
       " 'shear_v_0to1__time_avg__9km__ens_mean',\n",
       " 'shear_u_0to6__time_avg__9km__ens_mean',\n",
       " 'shear_v_0to6__time_avg__9km__ens_mean',\n",
       " 'shear_u_3to6__time_avg__9km__ens_mean',\n",
       " 'shear_v_3to6__time_avg__9km__ens_mean',\n",
       " 'srh_0to3__time_avg__9km__ens_mean',\n",
       " 'cape_ml__time_avg__9km__ens_mean',\n",
       " 'cin_ml__time_avg__9km__ens_mean',\n",
       " 'stp__time_avg__9km__ens_mean',\n",
       " 'scp__time_avg__9km__ens_mean',\n",
       " 'mid_level_lapse_rate__time_avg__9km__ens_std',\n",
       " 'low_level_lapse_rate__time_avg__9km__ens_std',\n",
       " 'shear_u_0to1__time_avg__9km__ens_std',\n",
       " 'shear_v_0to1__time_avg__9km__ens_std',\n",
       " 'shear_u_0to6__time_avg__9km__ens_std',\n",
       " 'shear_v_0to6__time_avg__9km__ens_std',\n",
       " 'shear_u_3to6__time_avg__9km__ens_std',\n",
       " 'shear_v_3to6__time_avg__9km__ens_std',\n",
       " 'srh_0to3__time_avg__9km__ens_std',\n",
       " 'cape_ml__time_avg__9km__ens_std',\n",
       " 'cin_ml__time_avg__9km__ens_std',\n",
       " 'stp__time_avg__9km__ens_std',\n",
       " 'scp__time_avg__9km__ens_std']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the Columns that I added if original=true\n",
    "X=X.drop(['NX','NY'], axis=1)\n",
    "original=False\n",
    "envOnly=False\n",
    "\n",
    "vardic={'ENS_VARS':['uh_2to5_instant',\n",
    "                     'uh_0to2_instant',\n",
    "                     'wz_0to2_instant',\n",
    "                     'comp_dz',\n",
    "                     'ws_80',\n",
    "                     'hailcast',\n",
    "                     'w_up',\n",
    "                     'okubo_weiss',\n",
    "                    ],\n",
    "        'ENV_VARS':['mid_level_lapse_rate', \n",
    "                    'low_level_lapse_rate', \n",
    "                    'shear_u_0to1', \n",
    "                    'shear_v_0to1', \n",
    "                    'shear_u_0to6', \n",
    "                    'shear_v_0to6',\n",
    "                    'shear_u_3to6', \n",
    "                    'shear_v_3to6',\n",
    "                    'srh_0to3',\n",
    "                    'cape_ml', \n",
    "                    'cin_ml', \n",
    "                    'stp',\n",
    "                    'scp',]}\n",
    "if original:\n",
    "    X=X[[col for col in X.columns if 'IQR' not in col]]\n",
    "    X=X[[col for col in X.columns if '2nd' not in col]]\n",
    "    X=X[[col for col in X.columns if '16th' not in col]]\n",
    "    #Mean of intrastorm vars\n",
    "   \n",
    "    stormcols=np.array([])\n",
    "    for strmvar in vardic['ENS_VARS']:\n",
    "        stormcols=np.append(stormcols, [col for col in X.columns if 'mean' in col and strmvar in col] ) #Every column w/ mean of intrastorm vars\n",
    "\n",
    "    X=X.drop(stormcols, axis=1)\n",
    "if envOnly:\n",
    "    stormcols=np.array([])\n",
    "    for strmvar in vardic['ENS_VARS']:\n",
    "        stormcols=np.append(stormcols, [col for col in X.columns if strmvar in col]) #Every column name that has a storm var\n",
    "    X=X.drop(stormcols, axis=1) #Drops all IS columns\n",
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a02dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1117741, 198)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78942a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/100 [02:39<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m train_dates \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun Date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     53\u001b[0m clf \u001b[38;5;241m=\u001b[39m CalibratedPipelineHyperOptCV(base_estimator\u001b[38;5;241m=\u001b[39mbase_estimator, \n\u001b[1;32m     54\u001b[0m                                    param_grid\u001b[38;5;241m=\u001b[39mparam_grid, \n\u001b[1;32m     55\u001b[0m                                    scaler\u001b[38;5;241m=\u001b[39mscaler, \n\u001b[1;32m     56\u001b[0m                                    resample\u001b[38;5;241m=\u001b[39mresample, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[1;32m     57\u001b[0m                                    cv_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdates\u001b[39m\u001b[38;5;124m'\u001b[39m: train_dates, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_splits\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_size\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m20\u001b[39m}, \n\u001b[1;32m     58\u001b[0m                                    hyperopt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m save_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVarga_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_original.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m clf\u001b[38;5;241m.\u001b[39msave(save_name)\n",
      "File \u001b[0;32m~/python_packages/ml_workflow/ml_workflow/calibrated_pipeline_hyperopt_cv.py:237\u001b[0m, in \u001b[0;36mCalibratedPipelineHyperOptCV.fit\u001b[0;34m(self, X, y, params)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Perform cross validation on the base estimator (no calibration!)\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_best_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m params\n",
      "File \u001b[0;32m~/python_packages/ml_workflow/ml_workflow/calibrated_pipeline_hyperopt_cv.py:356\u001b[0m, in \u001b[0;36mCalibratedPipelineHyperOptCV._find_best_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m\"\"\"Find the best hyperparameters using the hyperopt package\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Using early stopping in the error minimization. Need to have 1% drop in loss every 8-10 count (varies)\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAX_EVALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_progress_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration_stop_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpercent_increase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Get the values of the optimal parameters\u001b[39;00m\n\u001b[1;32m    366\u001b[0m best_params \u001b[38;5;241m=\u001b[39m space_eval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid, best)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[0;32m~/python_packages/ml_workflow/ml_workflow/calibrated_pipeline_hyperopt_cv.py:420\u001b[0m, in \u001b[0;36mCalibratedPipelineHyperOptCV.objective\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    418\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# Perform cross validation on the base estimator (no calibration!) \u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m fit_estimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# The fit estimators were fit on the training folds within clf\u001b[39;00m\n\u001b[1;32m    424\u001b[0m scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorer(model,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[test,:], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[test], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorer_kwargs) \n\u001b[1;32m    425\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m model, (_, test) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fit_estimators_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my))]\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/python_packages/ml_workflow/ml_workflow/calibrated_pipeline_hyperopt_cv.py:57\u001b[0m, in \u001b[0;36m_fit\u001b[0;34m(estimator, X, y)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(estimator, X, y): \n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/imblearn/pipeline.py:272\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    271\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 272\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/daal4py/sklearn/_device_offload.py:88\u001b[0m, in \u001b[0;36msupport_usm_ndarray.<locals>.decorator.<locals>.wrapper_with_self\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_with_self\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/daal4py/sklearn/_device_offload.py:74\u001b[0m, in \u001b[0;36msupport_usm_ndarray.<locals>.decorator.<locals>.wrapper_impl\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m usm_iface \u001b[38;5;241m=\u001b[39m _extract_usm_iface(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m q, hostargs, hostkwargs \u001b[38;5;241m=\u001b[39m _get_host_inputs(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 74\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_run_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhostargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhostkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m usm_iface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__array_interface__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _copy_to_usm(q, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/daal4py/sklearn/_device_offload.py:65\u001b[0m, in \u001b[0;36m_run_on_device\u001b[0;34m(func, queue, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m sycl_context(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m queue\u001b[38;5;241m.\u001b[39msycl_device\u001b[38;5;241m.\u001b[39mis_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     63\u001b[0m                           host_offload_on_fail\u001b[38;5;241m=\u001b[39mhost_offload):\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_by_obj(obj, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_by_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/daal4py/sklearn/_device_offload.py:53\u001b[0m, in \u001b[0;36m_run_on_device.<locals>.dispatch_by_obj\u001b[0;34m(obj, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_by_obj\u001b[39m(obj, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/daal4py/sklearn/ensemble/_forest.py:668\u001b[0m, in \u001b[0;36mRandomForestClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;129m@support_usm_ndarray\u001b[39m()\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    Build a forest of trees from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    self : object\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fit_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/daal4py/sklearn/ensemble/_forest.py:350\u001b[0m, in \u001b[0;36m_fit_classifier\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    442\u001b[0m ]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/sklearn/tree/_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m ):\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vanilla/lib/python3.10/site-packages/sklearn/tree/_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    411\u001b[0m         splitter,\n\u001b[1;32m    412\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    418\u001b[0m     )\n\u001b[0;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scaler = 'standard'\n",
    "resample = 'under'\n",
    "name='ADAM'\n",
    "\n",
    "if name=='logistic':\n",
    "    base_estimator = LogisticRegression(solver='saga', penalty='elasticnet', max_iter=300, random_state=42)\n",
    "    #Param grid for LogReg\n",
    "    param_grid = {\n",
    "                'l1_ratio': hp.choice('l1_ratio', [0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 0.6, 0.8, 1.0]),\n",
    "                'C': hp.choice('C', [0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.35, 0.5, 0.62, 0.75, 0.87, 1.0]),\n",
    "            }\n",
    "elif name=='random':\n",
    "    base_estimator=sklearn.ensemble.RandomForestClassifier(random_state=42)\n",
    "    #Param Grid for RF\n",
    "    param_grid = {\n",
    "               'n_estimators' : hp.choice('n_estimators',[10, 25, 50, 100,150,300,400,500]), \n",
    "               'max_depth' : hp.choice('max_depth',[4, 6,8,10,15,20]),\n",
    "               'max_features' : hp.choice('max_features',[4,6,8,10,15,20,25,30]),\n",
    "               'min_samples_split' : hp.choice('min_samples_split',[4,6,8,10,15,20,25,50]),\n",
    "               'min_samples_leaf' : hp.choice('min_samples_leaf',[4,6,8,10,15,20,25,50]),\n",
    "            }\n",
    "elif name=='hist':\n",
    "    base_estimator=sklearn.ensemble.HistGradientBoostingClassifier(random_state=42, max_iter=150)\n",
    "    #Check performance of different loss functions?\n",
    "    #Param Grid for HGB\n",
    "    param_grid= {\n",
    "    'learning_rate': hp.choice('learning_rate',[0.0001, 0.001, 0.01, 0.1]),\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes',[5, 10, 20, 30, 40, 50]),\n",
    "    'max_depth': hp.choice('max_depth', [4, 6, 8, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf',[5,10,15,20,30, 40, 50]),\n",
    "    'l2_regularization': hp.choice('l2_regularization',[0.001, 0.01, 0.1]), #This one causes problems\n",
    "    'max_bins': hp.choice('max_bins',[15, 31, 63, 127])\n",
    "    \n",
    "            }\n",
    "\n",
    "elif name=='ADAM':\n",
    "    base_estimator=sklearn.ensemble.RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "               'n_estimators' : hp.choice('n_estimators',[200]), \n",
    "               'criterion' : hp.choice('criterion',['entropy']),\n",
    "                'max_depth' : hp.choice('max_depth',[15]),\n",
    "               'max_features' : hp.choice('max_features',[\"sqrt\"]),\n",
    "               #'min_samples_split' : hp.choice('min_samples_split',[4,6,8,10,15,20,25,50]),\n",
    "               'min_samples_leaf' : hp.choice('min_samples_leaf',[20]),\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dates = metadata['Run Date'].apply(str)\n",
    "\n",
    "clf = CalibratedPipelineHyperOptCV(base_estimator=base_estimator, \n",
    "                                   param_grid=param_grid, \n",
    "                                   scaler=scaler, \n",
    "                                   resample=resample, max_iter=50, \n",
    "                                   cv_kwargs = {'dates': train_dates, 'n_splits': 5, 'valid_size' : 20}, \n",
    "                                   hyperopt='tpe')\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "save_name = f'Varga_{target_col}_model_{name}_original.joblib'\n",
    "clf.save(save_name) # <- need to make sure this points to my directory\n",
    "#target_col='wind_severe_36km\n",
    "#names=['hist','logistic','random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supposed to use hyperopt='tke'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
