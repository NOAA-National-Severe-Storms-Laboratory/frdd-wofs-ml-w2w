{
 "cells": [
  {
   "cell_type": "raw",
   "id": "40cbe71f",
   "metadata": {},
   "source": [
    "# Machine Learning Xset \n",
    "\n",
    "1. Extract the target variables at multiple scales \n",
    "    - grid scale, 15 km, and 30 km (grid scale is 9 km) \n",
    "\n",
    "2. 3-grid point maximum filter for intra-storm and 3-grid point average for the environment. The X will be roughly 100 x 100 grid points. (Similar to Sobash et al. 2020) \n",
    "\n",
    "3. To limit the X to a few million examples, only sample up to 10% of domain. \n",
    "\n",
    "Feature Extraction \n",
    "-------------------------------------------\n",
    "Intra-storm variables: \n",
    "    (low-level UH, mid-level UH, )\n",
    "\n",
    "    - Time-maximum values\n",
    "    - Ensemble 90th (10th) percentile/Max (Min) (multipler = 2) \n",
    "    - Different maximum filter size \n",
    "        - grid scale, 15 km, 30 km (multipler=3) \n",
    "    - Amplitude statistics\n",
    "        - Maximum value from each ensemble member within a distance of the point of interest (2 grid point)\n",
    "        - Compute ensemble mean and standard deviation \n",
    "    \n",
    "    Total = n_intrastorm * (2*2*3) + (2*n_intrastorm)\n",
    "    E.g., n_intrastorm = 10 -> Total = 140 \n",
    "    \n",
    "Environmental variables:\n",
    "    (STP, SCP, etc) \n",
    "\n",
    "    - Time-average values (2-4 hr avg. & 4-6 hr max) (multipler = 2) \n",
    "    - Ensemble mean, standard deviation \n",
    "\n",
    "# Baseline Dataset \n",
    "* Compute hazard-specific baseline \n",
    "    - Tornado = UH \n",
    "    - Hail = HAILCAST \n",
    "    - Wind = 80-m wind speed \n",
    "    \n",
    "* NMEP field at multiple thresholds and neighborhood sizes \n",
    "    - Apprioprate threshold/neighborhood size is likely to be target variable scale dependent \n",
    "    - Also, it will be metric dependent (e.g., Flora et al. 2021). We should prioritize AUPDC since we can calibrate the predictions. \n",
    "    \n",
    "* Once the best predictions are solidified, then train the isotonic regression model to calibrate the predictions. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "330e65f6",
   "metadata": {},
   "source": [
    "Loken et al. \n",
    "* Time-average for environ, Time-max for intra-storm\n",
    "* Ensemble maximum, minimum, and standard deviation for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84eb0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from WoF_post.wofs.post.utils import (\n",
    "    save_dataset,\n",
    "    load_multiple_nc_files,\n",
    ")\n",
    "from glob import glob\n",
    "from scipy.ndimage import uniform_filter, maximum_filter \n",
    "from collections import ChainMap\n",
    "import numpy as np\n",
    "\n",
    "from WoF_post.wofs.verification.lsrs.get_storm_reports import StormReports\n",
    "from WoF_post.wofs.plotting.util import decompose_file_path\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e770f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_config = { 'ENS_VARS':  ['uh_2to5_instant',\n",
    "                            'uh_0to2_instant',\n",
    "                            'wz_0to2_instant',\n",
    "                            'comp_dz',\n",
    "                            'ws_80',\n",
    "                            'hailcast',\n",
    "                            'w_up',\n",
    "                            'okubo_weiss',\n",
    "                    ],\n",
    "             \n",
    "              'ENV_VARS' : ['mid_level_lapse_rate', \n",
    "                            'low_level_lapse_rate', \n",
    "                           ],\n",
    "             \n",
    "              'SVR_VARS': ['shear_u_0to1', \n",
    "                        'shear_v_0to1', \n",
    "                        'shear_u_0to6', \n",
    "                        'shear_v_0to6',\n",
    "                        'shear_u_3to6', \n",
    "                        'shear_v_3to6',\n",
    "                        'srh_0to3',\n",
    "                        'cape_ml', \n",
    "                        'cin_ml', \n",
    "                        'stp',\n",
    "                        'scp',\n",
    "                       ]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc037826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    \"\"\"Get the ENS, ENV, and SVR file paths for the 2-6 hr forecasts\"\"\"\n",
    "    # Load the X. \n",
    "    ens_files = glob(join(path,f'wofs_ENS_[2-7]*'))\n",
    "    ens_files.sort()\n",
    "    ens_files = ens_files[4:]\n",
    "    \n",
    "    svr_files = [f.replace('ENS', 'SVR') for f in ens_files]\n",
    "    env_files = [f.replace('ENS', 'ENV') for f in ens_files]\n",
    "    \n",
    "    return ens_files, env_files, svr_files\n",
    "    \n",
    "def load_dataset(path):\n",
    "    \"\"\"Load the 2-6 hr forecasts\"\"\"\n",
    "    ens_files, env_files, svr_files = get_files(path)\n",
    "    \n",
    "    coord_vars = [\"xlat\", \"xlon\", \"hgt\"]\n",
    "    X_strm, _, _, _  = load_multiple_nc_files(\n",
    "                ens_files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=ml_config['ENS_VARS'])\n",
    "\n",
    "    X_env, _, _, _  = load_multiple_nc_files(\n",
    "                env_files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=ml_config['ENV_VARS'])\n",
    "\n",
    "    X_svr, _, _, _ = load_multiple_nc_files(\n",
    "                svr_files, concat_dim=\"time\", coord_vars=coord_vars,  load_vars=ml_config['SVR_VARS'])\n",
    "\n",
    "    X_env = {**X_env, **X_svr}\n",
    "\n",
    "    X_env = {v : X_env[v][1] for v in X_env.keys()}\n",
    "    X_strm = {v : X_strm[v][1] for v in X_strm.keys()}\n",
    "    \n",
    "    return X_env, X_strm, ens_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322f7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridPointExtracter:\n",
    "    \"\"\"Upscale X, compute time-composites, compute ensemble statistics.\"\"\"\n",
    "    def __init__(self, ncfile, env_vars, strm_vars, upscale_size=3):\n",
    "        self._n_ens = 18\n",
    "        self._env_vars = env_vars\n",
    "        self._strm_vars = strm_vars\n",
    "        \n",
    "        self._upscale_size = upscale_size\n",
    "        self._SIZES = [1,3,5]\n",
    "        self._TARGET_SIZES = [1,2,4]\n",
    "        self._ncfile = ncfile\n",
    "        self._DX = 9 \n",
    "        \n",
    "        self._BASELINE_VARS = ['hailcast', 'uh_2to5_instant', 'ws_80']\n",
    "        \n",
    "        self._NMEP_THRESHS = {'hailcast' : [0.5, 0.75, 1.0, 1.25, 1.5], \n",
    "                             'uh_2to5_instant' : [50, 75, 100, 125, 150, 175, 200],\n",
    "                             'ws_80' : [30, 40, 50, 60], \n",
    "                            }\n",
    "        \n",
    "    def __call__(self, X_env, X_strm, predict=False):\n",
    "        # This X has had a 3-grid point gaussian smoother applied to it. \n",
    "        X_env_upscaled = {v  : self.upscaler(X_env[v], \n",
    "                                     func=uniform_filter,\n",
    "                                     size=self._upscale_size) for v in self._env_vars}\n",
    "        \n",
    "        # This X has had a 3-grid point maximum filter applied to it. \n",
    "        X_strm_upscaled = {v : self.upscaler(X_strm[v], \n",
    "                                     func=maximum_filter,\n",
    "                                     size=self._upscale_size) for v in self._strm_vars}\n",
    "        \n",
    "        # For the environment, \n",
    "        # 1. Time-average \n",
    "        # 2. Spatial ensemble statistics at different scales. \n",
    "        X_env_time_comp = self.calc_time_composite(X_env_upscaled, \n",
    "                                                    func=np.mean, name='time_avg', keys=self._env_vars)\n",
    "        \n",
    "        \n",
    "        X_env_stats = self.calc_spatial_ensemble_stats(X_env_time_comp, environ=True)\n",
    "        \n",
    "        # For the storm, \n",
    "        # 1. Time-maximum \n",
    "        # 2. Spatial ensemble statistics at different scales. \n",
    "        # 3. Amplitude Statistics\n",
    "        X_strm_time_comp = self.calc_time_composite(X_strm_upscaled, \n",
    "                                                    func=np.max, name='time_max', keys=self._strm_vars)\n",
    "        \n",
    "        X_strm_stats = self.calc_spatial_ensemble_stats(X_strm_time_comp, environ=False)\n",
    "        \n",
    "        X_all = {**X_strm_stats, **X_env_stats}\n",
    "        \n",
    "        if predict:\n",
    "            data = X_all\n",
    "        else:\n",
    "            # IF not predicting, then get the target values. \n",
    "            y = self.get_targets()\n",
    "            data = {**X_all, **y}\n",
    "\n",
    "        \n",
    "        # Stack the data and convert to dataframe. \n",
    "        data = {v : (['NY', 'NX'], data[v]) for v in data.keys()}\n",
    "        ds = xr.Dataset(data)\n",
    "        df = ds.stack(z=('NY', 'NX')).to_dataframe()\n",
    "        \n",
    "        # Convert target variable to binary.\n",
    "        ys = [f for f in df.columns if 'severe' in f]\n",
    "        \n",
    "        new_df = df.copy()\n",
    "        for y_var in ys:\n",
    "            new_df[y_var] = np.where(df[y_var]>0, 1, 0)\n",
    "        \n",
    "        # Add date and init time\n",
    "        comps = self._ncfile.split('/')\n",
    "        date, init_time = comps[-3], comps[-2]\n",
    "        new_df[\"Run Date\"] = [date]*len(df)\n",
    "        new_df[\"Init Time\"] = [init_time]*len(df)\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    def get_nmep(self, X, size):\n",
    "        \"\"\"Compute the NMEP baseline\"\"\"\n",
    "        X_nmep = {}\n",
    "        for v in self._BASELINE_VARS:\n",
    "            for t in self._NMEP_THRESHS[v]:\n",
    "                data = X[f'{v}__time_max__{self._DX*size}km']\n",
    "                data_bin = np.where(data>t,1,0)\n",
    "                ens_prob = np.mean(data_bin, axis=0)\n",
    "                X_nmep[f\"{v}__nmep_>{str(t).replace('.','_')}_{self._DX*size}km\"] = ens_prob\n",
    "                \n",
    "        return X_nmep \n",
    "\n",
    "    def get_targets(self):\n",
    "        \"\"\"Convert storm reports to a grid and apply different upscaling\"\"\"\n",
    "        comps = decompose_file_path(self._ncfile)\n",
    "        init_time = comps['VALID_DATE']+comps['INIT_TIME']\n",
    "        report = StormReports(init_time, \n",
    "            forecast_length=240,\n",
    "            err_window=15, \n",
    "            )\n",
    "\n",
    "        ds = xr.load_dataset(self._ncfile, decode_times=False)\n",
    "        report_ds = report.to_grid(dataset=ds, size=self._upscale_size)\n",
    "        \n",
    "        keys = list(report_ds.data_vars)\n",
    "        \n",
    "        y = {v : report_ds[v].values[::self._upscale_size, ::self._upscale_size] \n",
    "             for v in keys}\n",
    "        \n",
    "        # Upscale the targets. \n",
    "        y_final = [] \n",
    "        for size in self._TARGET_SIZES:\n",
    "            y_nghbrd = {f'{v}__{self._DX*size}km' : self.neighborhooder(y[v], \n",
    "                                                                      func=maximum_filter,\n",
    "                                                                     size=size, is_2d=True) for v in keys}\n",
    "            y_final.append(y_nghbrd)\n",
    "            \n",
    "        y_final = dict(ChainMap(*y_final)) \n",
    "        \n",
    "        return y_final\n",
    "        \n",
    "    def neighborhooder(self, X, func, size, is_2d=False):\n",
    "        \"\"\"Apply neighborhood to the X.\"\"\"\n",
    "        new_X = X.copy()\n",
    "        if is_2d:\n",
    "            for n in range(self._n_ens):\n",
    "                new_X[:,:] = func(X[:,:], size)\n",
    "        else:\n",
    "            for n in range(self._n_ens):\n",
    "                new_X[n,:,:] = func(X[n,:,:], size)\n",
    "    \n",
    "        return new_X \n",
    "    \n",
    "    def upscaler(self, X, func, size):\n",
    "        \"\"\"Applies a spatial filter per ensemble member and then \n",
    "        subsamples the grid to reduce the number of grid points.\"\"\"\n",
    "        new_X = X.copy()\n",
    "        for n in range(self._n_ens):\n",
    "            new_X[:,n,:,:] = func(X[:,n,:,:], size)\n",
    "        return new_X[:,:,::size, ::size]\n",
    "    \n",
    "    def calc_time_composite(self, X, func, name, keys):\n",
    "        \"\"\"Compute the time-maximum or time-average\"\"\"\n",
    "        X_time_comp = {f'{v}__{name}' : func(X[v], axis=0) for v in keys }\n",
    "        \n",
    "        return X_time_comp\n",
    "        \n",
    "    def calc_spatial_ensemble_stats(self, X, environ=True):\n",
    "        \"\"\"Compute the spatial ensemble mean and standard deviation if environ = True,\n",
    "        else compute the ensemble 90th. Ensemble statistics are computed in multiple different \n",
    "        neighborhood sizes\"\"\"\n",
    "        \n",
    "        keys = X.keys()\n",
    "        \n",
    "        X_final = []\n",
    "        \n",
    "        for size in self._SIZES:\n",
    "            if environ:\n",
    "                X_nghbrd = {f'{v}__{self._DX*size}km' : self.neighborhooder(X[v], \n",
    "                                                                      func=uniform_filter,\n",
    "                                                                     size=size) for v in keys}\n",
    "                \n",
    "                X_ens_mean = {f'{v}__ens_mean' : np.mean(X_nghbrd[v], axis=0) for v in X_nghbrd.keys()}\n",
    "                X_ens_std = {f'{v}__ens_std' : np.std(X_nghbrd[v], axis=0, ddof=1) for v in X_nghbrd.keys()}   \n",
    "                X_ens_stats = {**X_ens_mean, **X_ens_std}\n",
    "            \n",
    "            else:\n",
    "                X_nghbrd = {f'{v}__{self._DX*size}km' : self.neighborhooder(X[v], \n",
    "                                                                      func=maximum_filter,\n",
    "                                                                     size=size) for v in keys}\n",
    "                \n",
    "                X_ens_90th = {f'{v}__ens_90th' : np.percentile(X_nghbrd[v],\n",
    "                                                                90, axis=0) for v in X_nghbrd.keys()} \n",
    "                \n",
    "                # Compute the baseline stuff. \n",
    "                X_baseline = self.get_nmep(X_nghbrd, size)\n",
    "\n",
    "                X_ens_stats = {**X_baseline, **X_ens_90th}\n",
    "            \n",
    "            X_final.append(X_ens_stats)\n",
    "            \n",
    "        X_final = dict(ChainMap(*X_final))    \n",
    "            \n",
    "        return X_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784d1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampler(y):\n",
    "    pos_percent = 0.5 \n",
    "    neg_percent = 0.1\n",
    "    \n",
    "    pos_inds = np.where(y>0)[0]\n",
    "    neg_inds = np.where(y==0)[0]\n",
    "    \n",
    "    if len(pos_inds) > 0:\n",
    "        pos_inds_sub = np.random.choice(pos_inds, size=int(pos_percent*(len(pos_inds))), replace=False)\n",
    "    else:\n",
    "        pod_inds_sub = [] \n",
    "    \n",
    "    neg_inds_sub = np.random.choice(neg_inds, size=int(neg_percent*(len(neg_inds))), replace=False)\n",
    "\n",
    "    inds = np.concatenate([pos_inds_sub, neg_inds_sub])\n",
    "    \n",
    "    return inds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9420e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow script. \n",
    "\n",
    "def worker(path):\n",
    "    X_env, X_strm, ncfile  = load_dataset(path)\n",
    "    extracter = GridPointExtracter(ncfile, env_vars=X_env.keys(), strm_vars=X_strm.keys())\n",
    "    df = extracter(X_env, X_strm)\n",
    "\n",
    "    ys = [f for f in df.columns if 'severe' in f]\n",
    "    y_df = df[ys].sum(axis='columns')\n",
    "\n",
    "    inds = subsampler(y_df)\n",
    "\n",
    "    df_sub = df.iloc[inds, :]\n",
    "    df_sub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_sub.to_feather(join(path, 'wofs_ML2TO6.feather'))\n",
    "    \n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc107a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2017-2020 dataset, loading Storm Data...\n"
     ]
    }
   ],
   "source": [
    "base_path = '/work/mflora/SummaryFiles'\n",
    "df_sub = worker(join(base_path, '20190520', '2200'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9bb2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4251700680272109"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_sub['wind_severe__9km'])*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28dd6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hailcast__nmep_>0_5_45km',\n",
       " 'hailcast__nmep_>0_75_45km',\n",
       " 'hailcast__nmep_>1_0_45km',\n",
       " 'hailcast__nmep_>1_25_45km',\n",
       " 'hailcast__nmep_>1_5_45km',\n",
       " 'uh_2to5_instant__nmep_>50_45km',\n",
       " 'uh_2to5_instant__nmep_>75_45km',\n",
       " 'uh_2to5_instant__nmep_>100_45km',\n",
       " 'uh_2to5_instant__nmep_>125_45km',\n",
       " 'uh_2to5_instant__nmep_>150_45km',\n",
       " 'uh_2to5_instant__nmep_>175_45km',\n",
       " 'uh_2to5_instant__nmep_>200_45km',\n",
       " 'ws_80__nmep_>30_45km',\n",
       " 'ws_80__nmep_>40_45km',\n",
       " 'ws_80__nmep_>50_45km',\n",
       " 'ws_80__nmep_>60_45km',\n",
       " 'uh_2to5_instant__time_max__45km__ens_90th',\n",
       " 'uh_0to2_instant__time_max__45km__ens_90th',\n",
       " 'wz_0to2_instant__time_max__45km__ens_90th',\n",
       " 'comp_dz__time_max__45km__ens_90th',\n",
       " 'ws_80__time_max__45km__ens_90th',\n",
       " 'hailcast__time_max__45km__ens_90th',\n",
       " 'w_up__time_max__45km__ens_90th',\n",
       " 'okubo_weiss__time_max__45km__ens_90th',\n",
       " 'hailcast__nmep_>0_5_27km',\n",
       " 'hailcast__nmep_>0_75_27km',\n",
       " 'hailcast__nmep_>1_0_27km',\n",
       " 'hailcast__nmep_>1_25_27km',\n",
       " 'hailcast__nmep_>1_5_27km',\n",
       " 'uh_2to5_instant__nmep_>50_27km',\n",
       " 'uh_2to5_instant__nmep_>75_27km',\n",
       " 'uh_2to5_instant__nmep_>100_27km',\n",
       " 'uh_2to5_instant__nmep_>125_27km',\n",
       " 'uh_2to5_instant__nmep_>150_27km',\n",
       " 'uh_2to5_instant__nmep_>175_27km',\n",
       " 'uh_2to5_instant__nmep_>200_27km',\n",
       " 'ws_80__nmep_>30_27km',\n",
       " 'ws_80__nmep_>40_27km',\n",
       " 'ws_80__nmep_>50_27km',\n",
       " 'ws_80__nmep_>60_27km',\n",
       " 'uh_2to5_instant__time_max__27km__ens_90th',\n",
       " 'uh_0to2_instant__time_max__27km__ens_90th',\n",
       " 'wz_0to2_instant__time_max__27km__ens_90th',\n",
       " 'comp_dz__time_max__27km__ens_90th',\n",
       " 'ws_80__time_max__27km__ens_90th',\n",
       " 'hailcast__time_max__27km__ens_90th',\n",
       " 'w_up__time_max__27km__ens_90th',\n",
       " 'okubo_weiss__time_max__27km__ens_90th',\n",
       " 'hailcast__nmep_>0_5_9km',\n",
       " 'hailcast__nmep_>0_75_9km',\n",
       " 'hailcast__nmep_>1_0_9km',\n",
       " 'hailcast__nmep_>1_25_9km',\n",
       " 'hailcast__nmep_>1_5_9km',\n",
       " 'uh_2to5_instant__nmep_>50_9km',\n",
       " 'uh_2to5_instant__nmep_>75_9km',\n",
       " 'uh_2to5_instant__nmep_>100_9km',\n",
       " 'uh_2to5_instant__nmep_>125_9km',\n",
       " 'uh_2to5_instant__nmep_>150_9km',\n",
       " 'uh_2to5_instant__nmep_>175_9km',\n",
       " 'uh_2to5_instant__nmep_>200_9km',\n",
       " 'ws_80__nmep_>30_9km',\n",
       " 'ws_80__nmep_>40_9km',\n",
       " 'ws_80__nmep_>50_9km',\n",
       " 'ws_80__nmep_>60_9km',\n",
       " 'uh_2to5_instant__time_max__9km__ens_90th',\n",
       " 'uh_0to2_instant__time_max__9km__ens_90th',\n",
       " 'wz_0to2_instant__time_max__9km__ens_90th',\n",
       " 'comp_dz__time_max__9km__ens_90th',\n",
       " 'ws_80__time_max__9km__ens_90th',\n",
       " 'hailcast__time_max__9km__ens_90th',\n",
       " 'w_up__time_max__9km__ens_90th',\n",
       " 'okubo_weiss__time_max__9km__ens_90th',\n",
       " 'mid_level_lapse_rate__time_avg__45km__ens_mean',\n",
       " 'low_level_lapse_rate__time_avg__45km__ens_mean',\n",
       " 'shear_u_0to1__time_avg__45km__ens_mean',\n",
       " 'shear_v_0to1__time_avg__45km__ens_mean',\n",
       " 'shear_u_0to6__time_avg__45km__ens_mean',\n",
       " 'shear_v_0to6__time_avg__45km__ens_mean',\n",
       " 'shear_u_3to6__time_avg__45km__ens_mean',\n",
       " 'shear_v_3to6__time_avg__45km__ens_mean',\n",
       " 'srh_0to3__time_avg__45km__ens_mean',\n",
       " 'cape_ml__time_avg__45km__ens_mean',\n",
       " 'cin_ml__time_avg__45km__ens_mean',\n",
       " 'stp__time_avg__45km__ens_mean',\n",
       " 'scp__time_avg__45km__ens_mean',\n",
       " 'mid_level_lapse_rate__time_avg__45km__ens_std',\n",
       " 'low_level_lapse_rate__time_avg__45km__ens_std',\n",
       " 'shear_u_0to1__time_avg__45km__ens_std',\n",
       " 'shear_v_0to1__time_avg__45km__ens_std',\n",
       " 'shear_u_0to6__time_avg__45km__ens_std',\n",
       " 'shear_v_0to6__time_avg__45km__ens_std',\n",
       " 'shear_u_3to6__time_avg__45km__ens_std',\n",
       " 'shear_v_3to6__time_avg__45km__ens_std',\n",
       " 'srh_0to3__time_avg__45km__ens_std',\n",
       " 'cape_ml__time_avg__45km__ens_std',\n",
       " 'cin_ml__time_avg__45km__ens_std',\n",
       " 'stp__time_avg__45km__ens_std',\n",
       " 'scp__time_avg__45km__ens_std',\n",
       " 'mid_level_lapse_rate__time_avg__27km__ens_mean',\n",
       " 'low_level_lapse_rate__time_avg__27km__ens_mean',\n",
       " 'shear_u_0to1__time_avg__27km__ens_mean',\n",
       " 'shear_v_0to1__time_avg__27km__ens_mean',\n",
       " 'shear_u_0to6__time_avg__27km__ens_mean',\n",
       " 'shear_v_0to6__time_avg__27km__ens_mean',\n",
       " 'shear_u_3to6__time_avg__27km__ens_mean',\n",
       " 'shear_v_3to6__time_avg__27km__ens_mean',\n",
       " 'srh_0to3__time_avg__27km__ens_mean',\n",
       " 'cape_ml__time_avg__27km__ens_mean',\n",
       " 'cin_ml__time_avg__27km__ens_mean',\n",
       " 'stp__time_avg__27km__ens_mean',\n",
       " 'scp__time_avg__27km__ens_mean',\n",
       " 'mid_level_lapse_rate__time_avg__27km__ens_std',\n",
       " 'low_level_lapse_rate__time_avg__27km__ens_std',\n",
       " 'shear_u_0to1__time_avg__27km__ens_std',\n",
       " 'shear_v_0to1__time_avg__27km__ens_std',\n",
       " 'shear_u_0to6__time_avg__27km__ens_std',\n",
       " 'shear_v_0to6__time_avg__27km__ens_std',\n",
       " 'shear_u_3to6__time_avg__27km__ens_std',\n",
       " 'shear_v_3to6__time_avg__27km__ens_std',\n",
       " 'srh_0to3__time_avg__27km__ens_std',\n",
       " 'cape_ml__time_avg__27km__ens_std',\n",
       " 'cin_ml__time_avg__27km__ens_std',\n",
       " 'stp__time_avg__27km__ens_std',\n",
       " 'scp__time_avg__27km__ens_std',\n",
       " 'mid_level_lapse_rate__time_avg__9km__ens_mean',\n",
       " 'low_level_lapse_rate__time_avg__9km__ens_mean',\n",
       " 'shear_u_0to1__time_avg__9km__ens_mean',\n",
       " 'shear_v_0to1__time_avg__9km__ens_mean',\n",
       " 'shear_u_0to6__time_avg__9km__ens_mean',\n",
       " 'shear_v_0to6__time_avg__9km__ens_mean',\n",
       " 'shear_u_3to6__time_avg__9km__ens_mean',\n",
       " 'shear_v_3to6__time_avg__9km__ens_mean',\n",
       " 'srh_0to3__time_avg__9km__ens_mean',\n",
       " 'cape_ml__time_avg__9km__ens_mean',\n",
       " 'cin_ml__time_avg__9km__ens_mean',\n",
       " 'stp__time_avg__9km__ens_mean',\n",
       " 'scp__time_avg__9km__ens_mean',\n",
       " 'mid_level_lapse_rate__time_avg__9km__ens_std',\n",
       " 'low_level_lapse_rate__time_avg__9km__ens_std',\n",
       " 'shear_u_0to1__time_avg__9km__ens_std',\n",
       " 'shear_v_0to1__time_avg__9km__ens_std',\n",
       " 'shear_u_0to6__time_avg__9km__ens_std',\n",
       " 'shear_v_0to6__time_avg__9km__ens_std',\n",
       " 'shear_u_3to6__time_avg__9km__ens_std',\n",
       " 'shear_v_3to6__time_avg__9km__ens_std',\n",
       " 'srh_0to3__time_avg__9km__ens_std',\n",
       " 'cape_ml__time_avg__9km__ens_std',\n",
       " 'cin_ml__time_avg__9km__ens_std',\n",
       " 'stp__time_avg__9km__ens_std',\n",
       " 'scp__time_avg__9km__ens_std',\n",
       " 'hail_severe__36km',\n",
       " 'wind_severe__36km',\n",
       " 'tornado_severe__36km',\n",
       " 'hail_sig_severe__36km',\n",
       " 'wind_sig_severe__36km',\n",
       " 'tornado_sig_severe__36km',\n",
       " 'hail_severe__18km',\n",
       " 'wind_severe__18km',\n",
       " 'tornado_severe__18km',\n",
       " 'hail_sig_severe__18km',\n",
       " 'wind_sig_severe__18km',\n",
       " 'tornado_sig_severe__18km',\n",
       " 'hail_severe__9km',\n",
       " 'wind_severe__9km',\n",
       " 'tornado_severe__9km',\n",
       " 'hail_sig_severe__9km',\n",
       " 'wind_sig_severe__9km',\n",
       " 'tornado_sig_severe__9km',\n",
       " 'Run Date',\n",
       " 'Init Time']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_sub.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976971e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind_severe__36km\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "np.mean(df['wind_severe__36km'])*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd76c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c484cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
